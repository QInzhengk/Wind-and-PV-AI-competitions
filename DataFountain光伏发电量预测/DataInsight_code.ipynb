{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# File system manangement\n",
    "import os\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train = pd.read_csv('public.train.csv')\n",
    "app_test = pd.read_csv('public.test.csv')\n",
    "\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "\n",
    "app_train_test.to_csv('data_prc.csv',index=False)\n",
    "\n",
    "\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "\n",
    "app_train.to_csv('train_prc.csv',index = False)\n",
    "app_test.to_csv('test_prc.csv',index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加与平均功率峰的值距离和平均功率峰值大小...\n",
      "添加平均功率的均值和方差...\n",
      "正在保存特征...\n",
      "特征构造完毕!总特征数量为： 25\n",
      "正在保存新的训练集和测试集...\n",
      "正在保存新的训练集和测试集...\n",
      "保存完毕\n"
     ]
    }
   ],
   "source": [
    "def add_former_power(features):\n",
    "\n",
    "    mean_power=features['平均功率']\n",
    "    former_power=np.zeros(features.shape[0])\n",
    "    for i in range(1,features.shape[0]):\n",
    "        former_power[i]=mean_power[i-1]\n",
    "    former_power[0]=mean_power[0]\n",
    "\n",
    "    features.insert(1,'former_power',former_power)\n",
    "    # features.to_csv('data/features_former_power.csv',index=None)\n",
    "\n",
    "    return features\n",
    "\n",
    "def add_dis2peak(data):\n",
    "    peaks_IDs_unchecked = [90, 309, 466, 686, 844, 1046, 1209, 1402, 1597, 1775, 1976, 2163, 2349, 2540,\n",
    "             2723, 2937, 3104, 3263, 3440, 3646, 3844, 3985, 4178, 4357, 4547, 4745, 4915,\n",
    "             5175, 5387, 5607, 5803, 6018, 6229, 6443, 6635, 6816, 7017, 7206, 7432, 7633,\n",
    "             7846, 8018, 8226, 8396, 8589, 8806, 8998, 9169, 9390, 9631, 9807, 10018, 10250,\n",
    "             10432, 10645, 10825, 10998, 11230, 11428, 11632, 11850, 12054, 12264, 12476,\n",
    "             12689, 12904, 13102, 13312, 13544, 13708, 13915, 14125, 14317, 14555, 14759,\n",
    "             14952, 15173, 15399, 15612, 15813, 16036, 16240, 16436, 16651, 16838, 17028,\n",
    "             17223, 17399, 17645, 17840]\n",
    "\n",
    "    peak_IDs = []\n",
    "    peak_index=[]\n",
    "    ID=data['ID']\n",
    "    for ID_i in peaks_IDs_unchecked:\n",
    "        for j, ID_j in enumerate(ID.values):\n",
    "            if ID_i >= ID_j and ID_i < ID[j+1]:\n",
    "                peak_IDs.append(ID_j)\n",
    "                peak_index.append(j)\n",
    "\n",
    "\n",
    "    dis2peak = []\n",
    "    peak_value=[]\n",
    "    mean_power=data['平均功率']\n",
    "    for id in data['ID']:\n",
    "        mindis = np.abs(id - peak_IDs[0])\n",
    "        peak_row=peak_index[0]\n",
    "        for i,peak_id in enumerate(peak_IDs):\n",
    "            if np.abs(id-peak_id)<mindis:\n",
    "                mindis=np.abs(id-peak_id)\n",
    "                peak_row=peak_index[i]\n",
    "        dis2peak.append(mindis)\n",
    "        peak_value.append(mean_power[peak_row])\n",
    "\n",
    "    data.insert(1, 'dis2peak', dis2peak )\n",
    "    data.insert(1, 'peak_value', peak_value )\n",
    "    return data\n",
    "\n",
    "def add_power_mean_std(data):\n",
    "\n",
    "    mean_power=[]\n",
    "    std_power=[]\n",
    "\n",
    "    for dis in enumerate(data['dis2peak']):\n",
    "        mean_power_i=np.mean(data[data['dis2peak']==dis[1]]['平均功率'])\n",
    "        mean_power.append(mean_power_i)\n",
    "        std_power_i=np.std(data[data['dis2peak']==dis[1]]['平均功率'])\n",
    "        std_power.append(std_power_i)\n",
    "\n",
    "    data.insert(17,'mean_power',mean_power)\n",
    "    data.insert(17,'std_power',std_power)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_mean_board_temperature(data,T=210):\n",
    "\n",
    "    board_temperature=data['板温']\n",
    "    mean_board_temperature=[]\n",
    "\n",
    "    for i in range(len(board_temperature)):\n",
    "        temperature_i=[]\n",
    "        if i<len(board_temperature)-T:\n",
    "            temperature_i=np.sum(board_temperature[i:i+T])*1.0/T\n",
    "        else:\n",
    "            temperature_i=np.sum(board_temperature[i-T:i])*1.0/T\n",
    "        mean_board_temperature.append(temperature_i)\n",
    "    data.insert(1,'mean_board_temp',mean_board_temperature)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_wind(data,T=20):\n",
    "\n",
    "    wind0=list(data['风速']*data['风向'])\n",
    "    wind=[]\n",
    "    for i in range(len(data['风向'])):\n",
    "        wind_i=[]\n",
    "        if i<T/2:\n",
    "            wind_i=np.sum(wind0[i:i+T])*1.0/T\n",
    "        elif i<len(data['风向'])-T/2:\n",
    "            wind_i=np.sum(wind0[i-int(T/2):i+int(T/2)])*1.0/T\n",
    "        else:\n",
    "            wind_i=np.sum(wind0[i-T:i])*1.0/T\n",
    "        wind.append(wind_i)\n",
    "    data.insert(17,'wind',wind0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_I_eta(data):\n",
    "\n",
    "    data.insert(6,'I_eta',data['光照强度']*data['转换效率'])\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_P_eta(data):\n",
    "    data.insert(16,'P_eta',data['转换效率']*data['平均功率'])\n",
    "    return data\n",
    "\n",
    "def add_temp_diff(data):\n",
    "    data.insert(10,'temp_diff',data['板温']-data['现场温度'])\n",
    "    return data\n",
    "\n",
    "def add_idc(data):\n",
    "    data.insert(11,'idc_A',data['电流A']/(data['转换效率A']+0.001))\n",
    "    data.insert(11,'idc_B',data['电流B']/(data['转换效率B']+0.001))\n",
    "    data.insert(11,'idc_C',data['电流C']/(data['转换效率C']+0.001))\n",
    "    return data\n",
    "\n",
    "def add_vdc(data):\n",
    "    data.insert(11,'vdc_A',data['电压A']/(data['转换效率A']+0.001))\n",
    "    data.insert(11,'vdc_B',data['电压B']/(data['转换效率B']+0.001))\n",
    "    data.insert(11,'vdc_C',data['电压C']/(data['转换效率C']+0.001))\n",
    "    return data\n",
    "\n",
    "def add_temp_diff_light(data):\n",
    "    data.insert(10,'温差乘以光强',np.abs((data['板温']-data['现场温度'])*data['光照强度']))\n",
    "    return data\n",
    "\n",
    "def add_PN_I(data):\n",
    "    I0=4.215*10**(-14)*np.exp(0.1539*data['板温'])\n",
    "    q=1.6*10**(-9)\n",
    "    K=0.86*10**(-4)\n",
    "    U=data['电压A']\n",
    "    T=data['板温']+273.15\n",
    "    Id=I0*(np.exp((q*U)/(K*T))-1)\n",
    "    data.insert(13,'PN_I',Id*10**16.5 )\n",
    "    return data\n",
    "\n",
    "def add_vdc_square(data):\n",
    "    data.insert(14,'vdc_A_square',(data['电压A']/(data['转换效率A']+0.001))**2)\n",
    "    data.insert(14,'vdc_B_square',(data['电压B']/(data['转换效率B']+0.001))**2)\n",
    "    data.insert(14,'vdc_C_square',(data['电压C']/(data['转换效率C']+0.001))**2)\n",
    "    return data\n",
    "\n",
    "def add_idc_square(data):\n",
    "    data.insert(17,'idc_A_square',(data['电流A']/(data['转换效率A']+0.001))**2)\n",
    "    data.insert(17,'idc_B_square',(data['电流B']/(data['转换效率B']+0.001))**2)\n",
    "    data.insert(17,'idc_C_square',(data['电流C']/(data['转换效率C']+0.001))**2)\n",
    "    return data\n",
    "\n",
    "def add_poly_features(data,column_names):\n",
    "    features=data[column_names]\n",
    "    rest_features=data.drop(column_names,axis=1)\n",
    "    poly_transformer=PolynomialFeatures(degree=2,interaction_only=False,include_bias=False)\n",
    "    poly_features=pd.DataFrame(poly_transformer.fit_transform(features),columns=poly_transformer.get_feature_names(column_names))\n",
    "\n",
    "    for col in poly_features.columns:\n",
    "        rest_features.insert(1,col,poly_features[col])\n",
    "\n",
    "    return rest_features\n",
    "\n",
    "def do_feature_project(features):\n",
    "\n",
    "\n",
    "\n",
    "    print('添加与平均功率峰的值距离和平均功率峰值大小...')\n",
    "    features=add_dis2peak(features)\n",
    "\n",
    "    print('添加平均功率的均值和方差...')\n",
    "    features=add_power_mean_std(features)\n",
    "\n",
    "\n",
    "    print('正在保存特征...')\n",
    "    features.to_csv('features.csv',index=None)\n",
    "\n",
    "    print('特征构造完毕!总特征数量为：',features.shape[1])\n",
    "\n",
    "    return features\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    train_data=pd.read_csv('train_prc.csv')\n",
    "    train_ID=train_data['ID']\n",
    "    test_data=pd.read_csv('test_prc.csv')\n",
    "    test_ID=test_data['ID']\n",
    "\n",
    "    features=pd.read_csv('data_prc.csv')\n",
    "    # print(features.head(10))\n",
    "\n",
    "    features=do_feature_project(features)\n",
    "    # features=pd.read_csv('data/features.csv')\n",
    "\n",
    "    train_features=features[features['ID'].isin(train_ID)]\n",
    "    train_features.reset_index(drop=True)\n",
    "    #train_features.insert(train_features.shape[1],'发电量',train_data['发电量'].values)\n",
    "    train_data=train_features\n",
    "\n",
    "    test_data=features[features['ID'].isin(test_ID)]\n",
    "    test_data.reset_index(drop=True)\n",
    "\n",
    "    print('正在保存新的训练集和测试集...')\n",
    "    train_data.to_csv('train_all_features2.csv',index=None)\n",
    "    test_data.to_csv('test_all_features2.csv',index=None)\n",
    "    print('正在保存新的训练集和测试集...')\n",
    "    print('保存完毕')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (9000, 25)\n",
      "Testing data shape:  (8409, 25)\n",
      "Polynomial Features shape:  (9000, 21)\n",
      "Training data with polynomial features shape:  (9000, 23)\n",
      "Testing data with polynomial features shape:   (8409, 22)\n",
      "Training data with polynomial features shape:  (9000, 58)\n",
      "Testing data with polynomial features shape:   (8409, 57)\n",
      "Training Data Shape:  (9000, 56)\n",
      "Testing Data Shape:  (8409, 56)\n",
      "[0]\tvalidation_0-rmse:6.15994\tvalidation_1-rmse:6.18917\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 300 rounds.\n",
      "[600]\tvalidation_0-rmse:0.129653\tvalidation_1-rmse:0.080876\n",
      "[1200]\tvalidation_0-rmse:0.123968\tvalidation_1-rmse:0.051604\n",
      "[1800]\tvalidation_0-rmse:0.122311\tvalidation_1-rmse:0.039312\n",
      "[2400]\tvalidation_0-rmse:0.12134\tvalidation_1-rmse:0.031598\n",
      "[3000]\tvalidation_0-rmse:0.120846\tvalidation_1-rmse:0.025913\n",
      "[3600]\tvalidation_0-rmse:0.120501\tvalidation_1-rmse:0.021628\n",
      "[4200]\tvalidation_0-rmse:0.120248\tvalidation_1-rmse:0.018262\n",
      "[4800]\tvalidation_0-rmse:0.120104\tvalidation_1-rmse:0.015583\n",
      "[5400]\tvalidation_0-rmse:0.12\tvalidation_1-rmse:0.013387\n",
      "[6000]\tvalidation_0-rmse:0.11993\tvalidation_1-rmse:0.011539\n",
      "[6600]\tvalidation_0-rmse:0.119846\tvalidation_1-rmse:0.009992\n",
      "[7200]\tvalidation_0-rmse:0.119785\tvalidation_1-rmse:0.008688\n",
      "[7800]\tvalidation_0-rmse:0.119733\tvalidation_1-rmse:0.007593\n",
      "[8400]\tvalidation_0-rmse:0.119705\tvalidation_1-rmse:0.006651\n",
      "[9000]\tvalidation_0-rmse:0.119656\tvalidation_1-rmse:0.005823\n",
      "[9600]\tvalidation_0-rmse:0.119636\tvalidation_1-rmse:0.005116\n",
      "[10200]\tvalidation_0-rmse:0.119617\tvalidation_1-rmse:0.004502\n",
      "[10800]\tvalidation_0-rmse:0.119599\tvalidation_1-rmse:0.003979\n",
      "[11400]\tvalidation_0-rmse:0.119589\tvalidation_1-rmse:0.003512\n",
      "[12000]\tvalidation_0-rmse:0.11958\tvalidation_1-rmse:0.003106\n",
      "[12600]\tvalidation_0-rmse:0.119572\tvalidation_1-rmse:0.002753\n",
      "[13200]\tvalidation_0-rmse:0.119564\tvalidation_1-rmse:0.002444\n",
      "[13800]\tvalidation_0-rmse:0.119554\tvalidation_1-rmse:0.002165\n",
      "[14400]\tvalidation_0-rmse:0.119552\tvalidation_1-rmse:0.001923\n",
      "[15000]\tvalidation_0-rmse:0.119547\tvalidation_1-rmse:0.001711\n",
      "[15600]\tvalidation_0-rmse:0.119544\tvalidation_1-rmse:0.001524\n",
      "[15999]\tvalidation_0-rmse:0.119542\tvalidation_1-rmse:0.001412\n",
      "[0]\tvalidation_0-rmse:6.21095\tvalidation_1-rmse:6.17217\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 300 rounds.\n",
      "[600]\tvalidation_0-rmse:0.137034\tvalidation_1-rmse:0.082718\n",
      "[1200]\tvalidation_0-rmse:0.128696\tvalidation_1-rmse:0.054705\n",
      "[1800]\tvalidation_0-rmse:0.126128\tvalidation_1-rmse:0.041961\n",
      "[2400]\tvalidation_0-rmse:0.12497\tvalidation_1-rmse:0.033679\n",
      "[3000]\tvalidation_0-rmse:0.124381\tvalidation_1-rmse:0.027654\n",
      "[3600]\tvalidation_0-rmse:0.123974\tvalidation_1-rmse:0.023117\n",
      "[4200]\tvalidation_0-rmse:0.123718\tvalidation_1-rmse:0.0196\n",
      "[4800]\tvalidation_0-rmse:0.123563\tvalidation_1-rmse:0.016719\n",
      "[5400]\tvalidation_0-rmse:0.123434\tvalidation_1-rmse:0.014369\n",
      "[6000]\tvalidation_0-rmse:0.123309\tvalidation_1-rmse:0.01243\n",
      "[6600]\tvalidation_0-rmse:0.12323\tvalidation_1-rmse:0.010767\n",
      "[7200]\tvalidation_0-rmse:0.123146\tvalidation_1-rmse:0.00938\n",
      "[7800]\tvalidation_0-rmse:0.12311\tvalidation_1-rmse:0.008164\n",
      "[8400]\tvalidation_0-rmse:0.123071\tvalidation_1-rmse:0.007148\n",
      "[9000]\tvalidation_0-rmse:0.12303\tvalidation_1-rmse:0.006267\n",
      "[9600]\tvalidation_0-rmse:0.123009\tvalidation_1-rmse:0.005509\n",
      "[10200]\tvalidation_0-rmse:0.122985\tvalidation_1-rmse:0.004856\n",
      "[10800]\tvalidation_0-rmse:0.122971\tvalidation_1-rmse:0.004281\n",
      "[11400]\tvalidation_0-rmse:0.122959\tvalidation_1-rmse:0.003793\n",
      "[12000]\tvalidation_0-rmse:0.122942\tvalidation_1-rmse:0.003365\n",
      "[12600]\tvalidation_0-rmse:0.122931\tvalidation_1-rmse:0.002997\n",
      "[13200]\tvalidation_0-rmse:0.122923\tvalidation_1-rmse:0.002668\n",
      "[13800]\tvalidation_0-rmse:0.122915\tvalidation_1-rmse:0.002377\n",
      "[14400]\tvalidation_0-rmse:0.122908\tvalidation_1-rmse:0.002115\n",
      "[15000]\tvalidation_0-rmse:0.122904\tvalidation_1-rmse:0.001891\n",
      "[15600]\tvalidation_0-rmse:0.1229\tvalidation_1-rmse:0.001687\n",
      "[15999]\tvalidation_0-rmse:0.122897\tvalidation_1-rmse:0.001566\n",
      "[0]\tvalidation_0-rmse:6.16901\tvalidation_1-rmse:6.18615\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 300 rounds.\n",
      "[600]\tvalidation_0-rmse:0.113432\tvalidation_1-rmse:0.083039\n",
      "[1200]\tvalidation_0-rmse:0.105919\tvalidation_1-rmse:0.054859\n",
      "[1800]\tvalidation_0-rmse:0.103766\tvalidation_1-rmse:0.043088\n",
      "[2400]\tvalidation_0-rmse:0.102487\tvalidation_1-rmse:0.034332\n",
      "[3000]\tvalidation_0-rmse:0.101828\tvalidation_1-rmse:0.028176\n",
      "[3600]\tvalidation_0-rmse:0.101357\tvalidation_1-rmse:0.023657\n",
      "[4200]\tvalidation_0-rmse:0.10107\tvalidation_1-rmse:0.020109\n",
      "[4800]\tvalidation_0-rmse:0.100898\tvalidation_1-rmse:0.017269\n",
      "[5400]\tvalidation_0-rmse:0.10078\tvalidation_1-rmse:0.014814\n",
      "[6000]\tvalidation_0-rmse:0.100692\tvalidation_1-rmse:0.012814\n",
      "[6600]\tvalidation_0-rmse:0.10063\tvalidation_1-rmse:0.011056\n",
      "[7200]\tvalidation_0-rmse:0.100591\tvalidation_1-rmse:0.009579\n",
      "[7800]\tvalidation_0-rmse:0.100559\tvalidation_1-rmse:0.008341\n",
      "[8400]\tvalidation_0-rmse:0.100533\tvalidation_1-rmse:0.007286\n",
      "[9000]\tvalidation_0-rmse:0.10052\tvalidation_1-rmse:0.006385\n",
      "[9600]\tvalidation_0-rmse:0.100515\tvalidation_1-rmse:0.005608\n",
      "[10200]\tvalidation_0-rmse:0.100508\tvalidation_1-rmse:0.004931\n",
      "[10800]\tvalidation_0-rmse:0.100496\tvalidation_1-rmse:0.004341\n",
      "[11400]\tvalidation_0-rmse:0.100484\tvalidation_1-rmse:0.003826\n",
      "[12000]\tvalidation_0-rmse:0.100484\tvalidation_1-rmse:0.003391\n",
      "[12600]\tvalidation_0-rmse:0.100477\tvalidation_1-rmse:0.003003\n",
      "[13200]\tvalidation_0-rmse:0.100475\tvalidation_1-rmse:0.002662\n",
      "[13800]\tvalidation_0-rmse:0.100469\tvalidation_1-rmse:0.00236\n",
      "[14400]\tvalidation_0-rmse:0.100467\tvalidation_1-rmse:0.002101\n",
      "[15000]\tvalidation_0-rmse:0.100467\tvalidation_1-rmse:0.001875\n",
      "[15600]\tvalidation_0-rmse:0.100466\tvalidation_1-rmse:0.001671\n",
      "[15999]\tvalidation_0-rmse:0.100466\tvalidation_1-rmse:0.00155\n",
      "[0]\tvalidation_0-rmse:6.18756\tvalidation_1-rmse:6.17991\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 300 rounds.\n",
      "[600]\tvalidation_0-rmse:0.255119\tvalidation_1-rmse:0.068713\n",
      "[1200]\tvalidation_0-rmse:0.253001\tvalidation_1-rmse:0.044736\n",
      "[1800]\tvalidation_0-rmse:0.252373\tvalidation_1-rmse:0.035158\n",
      "[2400]\tvalidation_0-rmse:0.252022\tvalidation_1-rmse:0.028543\n",
      "[3000]\tvalidation_0-rmse:0.251753\tvalidation_1-rmse:0.023679\n",
      "[3600]\tvalidation_0-rmse:0.25156\tvalidation_1-rmse:0.019954\n",
      "[4200]\tvalidation_0-rmse:0.251445\tvalidation_1-rmse:0.016911\n",
      "[4800]\tvalidation_0-rmse:0.25137\tvalidation_1-rmse:0.014498\n",
      "[5400]\tvalidation_0-rmse:0.251317\tvalidation_1-rmse:0.01247\n",
      "[6000]\tvalidation_0-rmse:0.251257\tvalidation_1-rmse:0.010794\n",
      "[6600]\tvalidation_0-rmse:0.251218\tvalidation_1-rmse:0.009365\n",
      "[7200]\tvalidation_0-rmse:0.251205\tvalidation_1-rmse:0.008164\n",
      "[7800]\tvalidation_0-rmse:0.251162\tvalidation_1-rmse:0.007151\n",
      "[8400]\tvalidation_0-rmse:0.251154\tvalidation_1-rmse:0.006261\n",
      "[9000]\tvalidation_0-rmse:0.25115\tvalidation_1-rmse:0.005487\n",
      "[9600]\tvalidation_0-rmse:0.251148\tvalidation_1-rmse:0.004839\n",
      "[10200]\tvalidation_0-rmse:0.251136\tvalidation_1-rmse:0.004259\n",
      "[10800]\tvalidation_0-rmse:0.251119\tvalidation_1-rmse:0.003757\n",
      "[11400]\tvalidation_0-rmse:0.25112\tvalidation_1-rmse:0.003328\n",
      "[12000]\tvalidation_0-rmse:0.251112\tvalidation_1-rmse:0.002951\n",
      "[12600]\tvalidation_0-rmse:0.251112\tvalidation_1-rmse:0.002622\n",
      "[13200]\tvalidation_0-rmse:0.251108\tvalidation_1-rmse:0.002329\n",
      "[13800]\tvalidation_0-rmse:0.251105\tvalidation_1-rmse:0.002067\n",
      "[14400]\tvalidation_0-rmse:0.251101\tvalidation_1-rmse:0.001837\n",
      "[15000]\tvalidation_0-rmse:0.251103\tvalidation_1-rmse:0.00164\n",
      "[15600]\tvalidation_0-rmse:0.251099\tvalidation_1-rmse:0.001465\n",
      "[15999]\tvalidation_0-rmse:0.251099\tvalidation_1-rmse:0.00136\n",
      "Baseline metrics\n",
      "      fold     train\n",
      "0        0  0.167130\n",
      "1        1  0.166061\n",
      "2        2  0.166907\n",
      "3        3  0.166501\n",
      "4  overall  0.166650\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "app_train = pd.read_csv('train_all_features2.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_test = pd.read_csv('test_all_features2.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "'''may change ffill'''\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "app_train=app_train.drop(columns='peak_value')\n",
    "app_test=app_test.drop(columns='peak_value')\n",
    "\n",
    "\n",
    "'''a'''\n",
    "app_train['理论输出']=app_train['光照强度']*app_train['转换效率']\n",
    "app_test['理论输出']=app_test['光照强度']*app_test['转换效率']\n",
    "'''b'''\n",
    "app_train['温差']=app_train['板温']-app_train['现场温度']\n",
    "app_test['温差']=app_test['板温']-app_test['现场温度']\n",
    "'''c'''\n",
    "app_train['实际功率']=app_train['转换效率']*app_train['平均功率']\n",
    "app_test['实际功率']=app_test['转换效率']*app_test['平均功率']\n",
    "'''d'''\n",
    "#app_train['风力X风向']=app_train['风向']*app_train['风速']\n",
    "#app_test['风力X风向']=app_test['风向']*app_test['风速']\n",
    "\n",
    "app_train['实际温度']=app_train['转换效率']*app_train['现场温度']\n",
    "app_test['实际温度']=app_test['转换效率']*app_test['现场温度']\n",
    "'''开始瞎jb蒙'''\n",
    "app_train['电压差A']=app_train['电流A']-app_train['电流B']\n",
    "app_test['电压差A']=app_test['电流A']-app_test['电流B']\n",
    "\n",
    "app_train['cde']=app_train['电压A']/app_train['转换效率A']\n",
    "app_test['cde']=app_test['电压A']/app_test['转换效率A']\n",
    "app_train['cde1']=app_train['电压B']/app_train['转换效率B']\n",
    "app_test['cde1']=app_test['电压B']/app_test['转换效率B']\n",
    "app_train['cde2']=app_train['电压C']/app_train['转换效率C']\n",
    "app_test['cde2']=app_test['电压C']/app_test['转换效率C']\n",
    "\n",
    "#app_train['abk']=app_train['abk']*app_train['abk']\n",
    "#app_test['abk']=app_test['abk']*app_test['abk']\n",
    "#app_train['iuo']=app_train['光照强度']*np.cos((app_train['ID']))\n",
    "#app_test['iuo']=app_test['光照强度']*np.cos((app_test['ID']))\n",
    "app_train['cdex']=app_train['cde']*app_train['cde']\n",
    "app_test['cdex']=app_test['cde']*app_test['cde']\n",
    "app_train['cdex1']=app_train['cde1']*app_train['cde1']\n",
    "app_test['cdex1']=app_test['cde1']*app_test['cde1']\n",
    "app_train['cdex2']=app_train['cde2']*app_train['cde2']\n",
    "app_test['cdex2']=app_test['cde2']*app_test['cde2']\n",
    "\n",
    "# print(app_train['dis2peak'])\n",
    "app_train['C_1']=app_train['dis2peak']*app_train['dis2peak']\n",
    "app_test['C_1']=app_test['dis2peak']*app_test['dis2peak']\n",
    "\n",
    "app_train['C_2']=app_train['dis2peak']*app_train['光照强度']\n",
    "app_test['C_2']=app_test['dis2peak']*app_test['光照强度']\n",
    "\n",
    "\n",
    "# app_train['C_4']=app_train['功率A']/app_train['风速']\n",
    "# app_test['C_4']=app_test['功率A']/app_test['风速']\n",
    "\n",
    "# app_train['C_5']=app_train['功率B']/app_train['风速']\n",
    "# app_test['C_5']=app_test['功率B']/app_test['风速']\n",
    "#\n",
    "# app_train['C_6']=app_train['C_4']*app_train['C_4']\n",
    "# app_test['C_6']=app_test['C_4']*app_test['C_4']\n",
    "# app_train['C']=app_train['电流B']-app_train['电流C']\n",
    "# app_test['C']=app_test['电流B']-app_test['电流C']\n",
    "\n",
    "# app_train['C_3']=app_train['dis2peak']*app_train['平均功率']\n",
    "# app_test['C_3']=app_test['dis2peak']*app_test['平均功率']\n",
    "\n",
    "poly_features = app_train[['板温','现场温度','光照强度','风速','风向']]\n",
    "poly_features_test = app_test[['板温','现场温度','光照强度','风速','风向']]\n",
    "\n",
    "\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = app_train['发电量']\n",
    "\n",
    "\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "#print(poly_corrs)\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "''''''\n",
    "\n",
    "\n",
    "''''''\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['ID'] = app_train['ID']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'ID', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['ID'] = app_test['ID']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'ID', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_poly['发电量']=poly_target\n",
    "app_train=app_train_poly\n",
    "app_test=app_test_poly\n",
    "\n",
    "\n",
    "#app_train.to_csv('xgb_poly_timetrain.csv',index=False)\n",
    "#app_test.to_csv('xgb_poly_timetest.csv',index=False)\n",
    "\n",
    "print('Training data with polynomial features shape: ', poly_features.shape)\n",
    "print('Testing data with polynomial features shape:  ', poly_features_test.shape)\n",
    "print('Training data with polynomial features shape: ', app_train.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test.shape)\n",
    "\n",
    "\n",
    "app_train['ID']=train_id\n",
    "app_test['ID']=test_id\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "def my_scorer(y_true, y_predicted,X_test):\n",
    "    loss_train = np.sum((y_true - y_predicted)**2, axis=0) / (X_test.shape[0])  #RMSE\n",
    "    loss_train = loss_train **0.5\n",
    "    score = 1/(1+loss_train)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 4):\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['ID']\n",
    "    test_ids = test_features['ID']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['发电量']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['ID', '发电量'])\n",
    "    test_features = test_features.drop(columns = ['ID'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    train_predictions = np.zeros(features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    #valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = xgb.XGBRegressor(objective = 'reg:linear',n_estimators=16000,min_child_weight=1,num_leaves=20,\n",
    "                                   learning_rate = 0.01, max_depth=6,n_jobs=20,\n",
    "                                   subsample = 0.6, colsample_bytree = 0.4, colsample_bylevel = 1)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels,\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  early_stopping_rounds = 300, verbose = 600)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = 16000\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test_features)/ k_fold.n_splits\n",
    "        train_predictions += model.predict(features)/ k_fold.n_splits\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold      = model.predict(valid_features)/ k_fold.n_splits\n",
    "\n",
    "        # Record the best score\n",
    "        train_score =  my_scorer(valid_labels,out_of_fold,valid_features)\n",
    "\n",
    "        # valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'ID': test_ids, '发电量': test_predictions})\n",
    "    train_sub = pd.DataFrame({'ID': train_ids, '发电量': train_predictions})\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    #valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    #valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metric = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            })\n",
    "\n",
    "    return submission, feature_importances, metric,train_sub\n",
    "submission, fi, metric,train_sub = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metric)\n",
    "xgb2lgb_train = pd.read_csv('train_all_features2.csv')\n",
    "xgb2lgb_test = pd.read_csv('test_all_features2.csv')\n",
    "xgb2lgb_train['xgb发电量']=train_sub['发电量']\n",
    "xgb2lgb_test['xgb发电量']=submission['发电量']\n",
    "xgb2lgb_train.to_csv('xgb2lgb_train.csv',index=False)\n",
    "xgb2lgb_test.to_csv('xgb2lgb_test.csv', index = False)    ##  0.080844   0.84669334000\n",
    "                                                              ##  0.079003    0.84672240000\n",
    "                                                               #0.093215     0.8457\n",
    "                                                               #0.078988   0.84892\n",
    "                                                               #0.078988   0.84897390000\n",
    "                                                               #0.84902996000\n",
    "                                                               #0.066291  0.84906185000\n",
    "                                                               #0.065589  0.84901553000\n",
    "                                                               #0.068688  0.84901680000\n",
    "                                                               #0.166649  0.84985\n",
    "                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (9000, 26)\n",
      "Testing data shape:  (8409, 26)\n",
      "Polynomial Features shape:  (9000, 21)\n",
      "Training data with polynomial features shape:  (9000, 23)\n",
      "Testing data with polynomial features shape:   (8409, 22)\n",
      "Training data with polynomial features shape:  (9000, 63)\n",
      "Testing data with polynomial features shape:   (8409, 62)\n",
      "Training Data Shape:  (9000, 61)\n",
      "Testing Data Shape:  (8409, 61)\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.184332\ttrain's rmse: 0.186485\n",
      "[1200]\tvalid's rmse: 0.0526237\ttrain's rmse: 0.0563721\n",
      "[1800]\tvalid's rmse: 0.048828\ttrain's rmse: 0.047659\n",
      "[2400]\tvalid's rmse: 0.0483967\ttrain's rmse: 0.0432651\n",
      "[3000]\tvalid's rmse: 0.0480205\ttrain's rmse: 0.0397653\n",
      "[3600]\tvalid's rmse: 0.0477495\ttrain's rmse: 0.0367731\n",
      "[4200]\tvalid's rmse: 0.0474931\ttrain's rmse: 0.0341058\n",
      "[4800]\tvalid's rmse: 0.0471991\ttrain's rmse: 0.031743\n",
      "[5400]\tvalid's rmse: 0.0469728\ttrain's rmse: 0.0296853\n",
      "[6000]\tvalid's rmse: 0.0467087\ttrain's rmse: 0.0278889\n",
      "[6600]\tvalid's rmse: 0.0465896\ttrain's rmse: 0.026353\n",
      "[7200]\tvalid's rmse: 0.0464669\ttrain's rmse: 0.0249899\n",
      "[7800]\tvalid's rmse: 0.0463334\ttrain's rmse: 0.0236955\n",
      "[8400]\tvalid's rmse: 0.0462483\ttrain's rmse: 0.0225994\n",
      "[9000]\tvalid's rmse: 0.0461893\ttrain's rmse: 0.0215576\n",
      "[9600]\tvalid's rmse: 0.0461283\ttrain's rmse: 0.0206436\n",
      "[10200]\tvalid's rmse: 0.0461155\ttrain's rmse: 0.019798\n",
      "[10800]\tvalid's rmse: 0.0461089\ttrain's rmse: 0.0190256\n",
      "[11400]\tvalid's rmse: 0.0460969\ttrain's rmse: 0.0183081\n",
      "[12000]\tvalid's rmse: 0.0460967\ttrain's rmse: 0.017634\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11949]\tvalid's rmse: 0.0460933\ttrain's rmse: 0.0176923\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.188959\ttrain's rmse: 0.185318\n",
      "[1200]\tvalid's rmse: 0.0554471\ttrain's rmse: 0.055306\n",
      "[1800]\tvalid's rmse: 0.0510483\ttrain's rmse: 0.0467138\n",
      "[2400]\tvalid's rmse: 0.0503961\ttrain's rmse: 0.0422191\n",
      "[3000]\tvalid's rmse: 0.0498575\ttrain's rmse: 0.0387528\n",
      "[3600]\tvalid's rmse: 0.0494793\ttrain's rmse: 0.0359032\n",
      "[4200]\tvalid's rmse: 0.0491221\ttrain's rmse: 0.0334459\n",
      "[4800]\tvalid's rmse: 0.0488244\ttrain's rmse: 0.0312494\n",
      "[5400]\tvalid's rmse: 0.0486989\ttrain's rmse: 0.0294219\n",
      "[6000]\tvalid's rmse: 0.0486974\ttrain's rmse: 0.027845\n",
      "[6600]\tvalid's rmse: 0.048689\ttrain's rmse: 0.0264372\n",
      "[7200]\tvalid's rmse: 0.0486228\ttrain's rmse: 0.025158\n",
      "[7800]\tvalid's rmse: 0.0485704\ttrain's rmse: 0.0239218\n",
      "[8400]\tvalid's rmse: 0.0485365\ttrain's rmse: 0.0228273\n",
      "[9000]\tvalid's rmse: 0.0485195\ttrain's rmse: 0.0218336\n",
      "[9600]\tvalid's rmse: 0.0485146\ttrain's rmse: 0.0208973\n",
      "[10200]\tvalid's rmse: 0.0485024\ttrain's rmse: 0.0200371\n",
      "[10800]\tvalid's rmse: 0.0484875\ttrain's rmse: 0.0192382\n",
      "[11400]\tvalid's rmse: 0.0485208\ttrain's rmse: 0.0184795\n",
      "[12000]\tvalid's rmse: 0.0485228\ttrain's rmse: 0.0177564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10829]\tvalid's rmse: 0.0484777\ttrain's rmse: 0.0191977\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.180246\ttrain's rmse: 0.187553\n",
      "[1200]\tvalid's rmse: 0.0482073\ttrain's rmse: 0.0565374\n",
      "[1800]\tvalid's rmse: 0.0465847\ttrain's rmse: 0.0478077\n",
      "[2400]\tvalid's rmse: 0.0461071\ttrain's rmse: 0.0434791\n",
      "[3000]\tvalid's rmse: 0.0456759\ttrain's rmse: 0.0398563\n",
      "[3600]\tvalid's rmse: 0.0453973\ttrain's rmse: 0.0368767\n",
      "[4200]\tvalid's rmse: 0.0451666\ttrain's rmse: 0.0343909\n",
      "[4800]\tvalid's rmse: 0.0450436\ttrain's rmse: 0.0322341\n",
      "[5400]\tvalid's rmse: 0.0448554\ttrain's rmse: 0.0303482\n",
      "[6000]\tvalid's rmse: 0.0446802\ttrain's rmse: 0.0285974\n",
      "[6600]\tvalid's rmse: 0.0445774\ttrain's rmse: 0.0270213\n",
      "[7200]\tvalid's rmse: 0.044432\ttrain's rmse: 0.0255985\n",
      "[7800]\tvalid's rmse: 0.044364\ttrain's rmse: 0.0243499\n",
      "[8400]\tvalid's rmse: 0.0442856\ttrain's rmse: 0.0232212\n",
      "[9000]\tvalid's rmse: 0.0442334\ttrain's rmse: 0.0221825\n",
      "[9600]\tvalid's rmse: 0.0441906\ttrain's rmse: 0.0212272\n",
      "[10200]\tvalid's rmse: 0.0441785\ttrain's rmse: 0.0202754\n",
      "[10800]\tvalid's rmse: 0.0441405\ttrain's rmse: 0.0194261\n",
      "[11400]\tvalid's rmse: 0.0441189\ttrain's rmse: 0.0186428\n",
      "[12000]\tvalid's rmse: 0.0441286\ttrain's rmse: 0.0179079\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11567]\tvalid's rmse: 0.0441142\ttrain's rmse: 0.0184344\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.199903\ttrain's rmse: 0.180741\n",
      "[1200]\tvalid's rmse: 0.102409\ttrain's rmse: 0.040329\n",
      "[1800]\tvalid's rmse: 0.099588\ttrain's rmse: 0.0337289\n",
      "[2400]\tvalid's rmse: 0.0994867\ttrain's rmse: 0.0309196\n",
      "[3000]\tvalid's rmse: 0.0993034\ttrain's rmse: 0.0288595\n",
      "[3600]\tvalid's rmse: 0.0990903\ttrain's rmse: 0.0270929\n",
      "[4200]\tvalid's rmse: 0.0988493\ttrain's rmse: 0.0254848\n",
      "[4800]\tvalid's rmse: 0.0986122\ttrain's rmse: 0.024059\n",
      "[5400]\tvalid's rmse: 0.0983055\ttrain's rmse: 0.022807\n",
      "[6000]\tvalid's rmse: 0.0980873\ttrain's rmse: 0.0217062\n",
      "[6600]\tvalid's rmse: 0.0978473\ttrain's rmse: 0.0206593\n",
      "[7200]\tvalid's rmse: 0.0977323\ttrain's rmse: 0.0197653\n",
      "[7800]\tvalid's rmse: 0.0975293\ttrain's rmse: 0.0188923\n",
      "[8400]\tvalid's rmse: 0.0974306\ttrain's rmse: 0.0182047\n",
      "[9000]\tvalid's rmse: 0.0973482\ttrain's rmse: 0.0174596\n",
      "[9600]\tvalid's rmse: 0.0971942\ttrain's rmse: 0.0167532\n",
      "[10200]\tvalid's rmse: 0.0970236\ttrain's rmse: 0.0160909\n",
      "[10800]\tvalid's rmse: 0.0969044\ttrain's rmse: 0.0154822\n",
      "[11400]\tvalid's rmse: 0.0968014\ttrain's rmse: 0.0148955\n",
      "[12000]\tvalid's rmse: 0.0966932\ttrain's rmse: 0.0143522\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11991]\tvalid's rmse: 0.0966926\ttrain's rmse: 0.0143611\n",
      "Baseline metrics\n",
      "      fold     train     valid\n",
      "0        0  0.017692  0.046093\n",
      "1        1  0.019198  0.048478\n",
      "2        2  0.018434  0.044114\n",
      "3        3  0.014361  0.096693\n",
      "4  overall  0.017421  0.058844\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "app_train = pd.read_csv('xgb2lgb_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_test = pd.read_csv('xgb2lgb_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "app_train=app_train.drop(columns='peak_value')\n",
    "app_test=app_test.drop(columns='peak_value')\n",
    "\n",
    "\n",
    "\n",
    "app_train['理论输出']=app_train['光照强度']*app_train['转换效率']\n",
    "app_test['理论输出']=app_test['光照强度']*app_test['转换效率']\n",
    "\n",
    "app_train['温差']=app_train['板温']-app_train['现场温度']\n",
    "app_test['温差']=app_test['板温']-app_test['现场温度']\n",
    "\n",
    "app_train['实际功率']=app_train['转换效率']*app_train['平均功率']\n",
    "app_test['实际功率']=app_test['转换效率']*app_test['平均功率']\n",
    "\n",
    "#app_train['风力X风向']=app_train['风向']*app_train['风速']\n",
    "#app_test['风力X风向']=app_test['风向']*app_test['风速']\n",
    "\n",
    "app_train['实际温度']=app_train['转换效率']*app_train['现场温度']\n",
    "app_test['实际温度']=app_test['转换效率']*app_test['现场温度']\n",
    "\n",
    "app_train['电压差A']=app_train['电流A']-app_train['电流B']\n",
    "app_test['电压差A']=app_test['电流A']-app_test['电流B']\n",
    "\n",
    "app_train['cde']=app_train['电压A']/app_train['转换效率A']\n",
    "app_test['cde']=app_test['电压A']/app_test['转换效率A']\n",
    "app_train['cde1']=app_train['电压B']/app_train['转换效率B']\n",
    "app_test['cde1']=app_test['电压B']/app_test['转换效率B']\n",
    "app_train['cde2']=app_train['电压C']/app_train['转换效率C']\n",
    "app_test['cde2']=app_test['电压C']/app_test['转换效率C']\n",
    "\n",
    "#app_train['abk']=app_train['abk']*app_train['abk']\n",
    "#app_test['abk']=app_test['abk']*app_test['abk']\n",
    "#app_train['iuo']=app_train['光照强度']*np.cos((app_train['ID']))\n",
    "#app_test['iuo']=app_test['光照强度']*np.cos((app_test['ID']))\n",
    "app_train['cdex']=app_train['cde']*app_train['cde']\n",
    "app_test['cdex']=app_test['cde']*app_test['cde']\n",
    "app_train['cdex1']=app_train['cde1']*app_train['cde1']\n",
    "app_test['cdex1']=app_test['cde1']*app_test['cde1']\n",
    "app_train['cdex2']=app_train['cde2']*app_train['cde2']\n",
    "app_test['cdex2']=app_test['cde2']*app_test['cde2']\n",
    "\n",
    "# print(app_train['dis2peak'])\n",
    "app_train['C_1']=app_train['dis2peak']*app_train['dis2peak']\n",
    "app_test['C_1']=app_test['dis2peak']*app_test['dis2peak']\n",
    "\n",
    "app_train['C_2']=app_train['dis2peak']*app_train['光照强度']\n",
    "app_test['C_2']=app_test['dis2peak']*app_test['光照强度']\n",
    "\n",
    "app_train['C_3']=app_train['电流B']-app_train['电流C']\n",
    "app_test['C_3']=app_test['电流B']-app_test['电流C']\n",
    "\n",
    "\n",
    "# app_train['实际温度1']=app_train['转换效率']*app_train['风向']\n",
    "# app_test['实际温度1']=app_test['转换效率']*app_test['风向']\n",
    "\n",
    "app_train['C_4']=app_train['功率A']/app_train['风速']\n",
    "app_test['C_4']=app_test['功率A']/app_test['风速']\n",
    "\n",
    "app_train['C_5']=app_train['功率B']/app_train['风速']\n",
    "app_test['C_5']=app_test['功率B']/app_test['风速']\n",
    "\n",
    "app_train['C_6']=app_train['C_4']*app_train['C_4']\n",
    "app_test['C_6']=app_test['C_4']*app_test['C_4']\n",
    "\n",
    "# app_train['C_7']=app_train['功率A']*app_train['风向']\n",
    "# app_test['C_7']=app_test['功率A']*app_test['风向']\n",
    "\n",
    "# app_train['实际温度3']=app_train['功率C']/app_train['风速']\n",
    "# app_test['实际温度3']=app_test['功率C']/app_test['风速']\n",
    "#app_train['C_9']=app_train['风向']*app_train['转换效率A']\n",
    "#app_test['C_9']=app_test['风向']*app_test['转换效率A']\n",
    "\n",
    "# app_train['实际温度3']=app_train['功率B']*app_train['风速']\n",
    "# app_test['实际温度3']=app_test['功率B']*app_test['风速']\n",
    "\n",
    "# app_train['cde5']=app_train['cde4'] *app_train['cde4']\n",
    "# app_test['cde5']=app_test['cde4'] * app_test['cde4']\n",
    "\n",
    "# app_train['C_3']=app_train['电流C']-app_train['电流A']\n",
    "# app_test['C_3']=app_test['电流C']-app_test['电流A']\n",
    "\n",
    "poly_features = app_train[['板温','现场温度','光照强度','风速','风向']]\n",
    "poly_features_test = app_test[['板温','现场温度','光照强度','风速','风向']]\n",
    "\n",
    "\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = app_train['发电量']\n",
    "\n",
    "\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "#print(poly_corrs)\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "''''''\n",
    "\n",
    "\n",
    "''''''\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['ID'] = app_train['ID']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'ID', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['ID'] = app_test['ID']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'ID', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_poly['发电量']=poly_target\n",
    "app_train=app_train_poly\n",
    "app_test=app_test_poly\n",
    "\n",
    "\n",
    "#app_train.to_csv('xgb_poly_timetrain.csv',index=False)\n",
    "#app_test.to_csv('xgb_poly_timetest.csv',index=False)\n",
    "\n",
    "print('Training data with polynomial features shape: ', poly_features.shape)\n",
    "print('Testing data with polynomial features shape:  ', poly_features_test.shape)\n",
    "print('Training data with polynomial features shape: ', app_train.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test.shape)\n",
    "\n",
    "\n",
    "app_train['ID']=train_id\n",
    "app_test['ID']=test_id\n",
    "\n",
    "#app_train = app_train.drop(['1', '板温_y','现场温度_y', '光照强度_y', '风速_y', '风向_y'],axis=1)\n",
    "#app_test = app_test.drop(['1', '板温_y','现场温度_y', '光照强度_y', '风速_y', '风向_y'],axis=1)\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 4):\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['ID']\n",
    "    test_ids = test_features['ID']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['发电量']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['ID', '发电量'])\n",
    "    test_features = test_features.drop(columns = ['ID'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    train_predictions = np.zeros(features.shape[0])\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = lgb.LGBMRegressor(objective = 'regression',n_estimators=12000,min_child_samples=20,num_leaves=20,\n",
    "                                   learning_rate = 0.005, feature_fraction=0.8,\n",
    "                                   subsample = 0.5, n_jobs = -1, random_state = 50)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'rmse',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 2000, verbose = 600)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        train_predictions += model.predict(features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict(valid_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['rmse']\n",
    "        train_score = model.best_score_['train']['rmse']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'ID': test_ids, '发电量': test_predictions})\n",
    "    train_sub = pd.DataFrame({'ID': train_ids, '发电量': train_predictions})\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    #valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    #valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    valid_scores.append(np.mean(valid_scores))\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metric = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid':valid_scores})\n",
    "\n",
    "    return submission, feature_importances, metric,train_sub\n",
    "submission, fi, metric,train_sub = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metric)\n",
    "lgb2xgb_train = pd.read_csv('train_all_features2.csv')\n",
    "lgb2xgb_test = pd.read_csv('test_all_features2.csv')\n",
    "lgb2xgb_train['xgb发电量']=train_sub['发电量']\n",
    "lgb2xgb_test['xgb发电量']=submission['发电量']\n",
    "lgb2xgb_train.to_csv('lgb2xgb_train.csv',index=False)\n",
    "lgb2xgb_test.to_csv('lgb2xgb_test.csv', index = False)\n",
    "#submission.to_csv('poly_time_54f_test.csv', index = False)    ##  0.080844   0.84669334000\n",
    "                                               \n",
    "                                                               ##  0.079003    0.84672240000\n",
    "                                                               #0.093215     0.8457\n",
    "                                                               #0.078988   0.84892\n",
    "                                                               #0.078988   0.84897390000\n",
    "                                                               #0.84902996000\n",
    "                                                               #0.066291  0.84906185000\n",
    "                                                               #0.065589  0.84901553000\n",
    "                                                               #0.068688  0.84901680000\n",
    "                                                               #0.065880  0.8488367000\n",
    "                                                               #0.067978  0.84887385000\n",
    "                                                               #0.063314 0.84917360000\n",
    "                                                               #0.073647  0.84889290000\n",
    "                                                               #0.065066  0.84911215000\n",
    "                                                               #0.063880  0.84924483000\n",
    "                                                               #0.065388  0.84930140000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (9000, 26)\n",
      "Testing data shape:  (8409, 26)\n",
      "Polynomial Features shape:  (9000, 21)\n",
      "Training data with polynomial features shape:  (9000, 23)\n",
      "Testing data with polynomial features shape:   (8409, 22)\n",
      "Training data with polynomial features shape:  (9000, 66)\n",
      "Testing data with polynomial features shape:   (8409, 65)\n",
      "Training Data Shape:  (9000, 64)\n",
      "Testing Data Shape:  (8409, 64)\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[600]\tvalid's rmse: 0.19234\ttrain's rmse: 0.194957\n",
      "[1200]\tvalid's rmse: 0.0678536\ttrain's rmse: 0.0705824\n",
      "[1800]\tvalid's rmse: 0.0632425\ttrain's rmse: 0.0595645\n",
      "[2400]\tvalid's rmse: 0.062106\ttrain's rmse: 0.0533369\n",
      "[3000]\tvalid's rmse: 0.0614817\ttrain's rmse: 0.048236\n",
      "[3600]\tvalid's rmse: 0.061017\ttrain's rmse: 0.0439336\n",
      "[4200]\tvalid's rmse: 0.0606301\ttrain's rmse: 0.0403533\n",
      "[4800]\tvalid's rmse: 0.0603195\ttrain's rmse: 0.0372696\n",
      "[5400]\tvalid's rmse: 0.0600957\ttrain's rmse: 0.0345549\n",
      "[6000]\tvalid's rmse: 0.059884\ttrain's rmse: 0.0320917\n",
      "[6600]\tvalid's rmse: 0.0597008\ttrain's rmse: 0.0299319\n",
      "[7200]\tvalid's rmse: 0.059577\ttrain's rmse: 0.0279963\n",
      "[7800]\tvalid's rmse: 0.0594385\ttrain's rmse: 0.0262754\n",
      "[8400]\tvalid's rmse: 0.059359\ttrain's rmse: 0.0247713\n",
      "[9000]\tvalid's rmse: 0.0592832\ttrain's rmse: 0.0233645\n",
      "[9600]\tvalid's rmse: 0.0592214\ttrain's rmse: 0.0221012\n",
      "[10200]\tvalid's rmse: 0.059172\ttrain's rmse: 0.0209542\n",
      "[10800]\tvalid's rmse: 0.0591255\ttrain's rmse: 0.0199061\n",
      "[11400]\tvalid's rmse: 0.0591195\ttrain's rmse: 0.018945\n",
      "[12000]\tvalid's rmse: 0.0590971\ttrain's rmse: 0.0180469\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11817]\tvalid's rmse: 0.0590947\ttrain's rmse: 0.0183305\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[600]\tvalid's rmse: 0.198917\ttrain's rmse: 0.193681\n",
      "[1200]\tvalid's rmse: 0.0728422\ttrain's rmse: 0.0692919\n",
      "[1800]\tvalid's rmse: 0.0673446\ttrain's rmse: 0.0580503\n",
      "[2400]\tvalid's rmse: 0.0664111\ttrain's rmse: 0.0518344\n",
      "[3000]\tvalid's rmse: 0.0659138\ttrain's rmse: 0.046908\n",
      "[3600]\tvalid's rmse: 0.0655229\ttrain's rmse: 0.0428329\n",
      "[4200]\tvalid's rmse: 0.0651889\ttrain's rmse: 0.0392412\n",
      "[4800]\tvalid's rmse: 0.064911\ttrain's rmse: 0.036214\n",
      "[5400]\tvalid's rmse: 0.0647727\ttrain's rmse: 0.0335297\n",
      "[6000]\tvalid's rmse: 0.0646274\ttrain's rmse: 0.0311652\n",
      "[6600]\tvalid's rmse: 0.0645089\ttrain's rmse: 0.0290865\n",
      "[7200]\tvalid's rmse: 0.0644186\ttrain's rmse: 0.0272688\n",
      "[7800]\tvalid's rmse: 0.064387\ttrain's rmse: 0.0256244\n",
      "[8400]\tvalid's rmse: 0.064373\ttrain's rmse: 0.024161\n",
      "[9000]\tvalid's rmse: 0.064362\ttrain's rmse: 0.0228244\n",
      "[9600]\tvalid's rmse: 0.0643573\ttrain's rmse: 0.0216217\n",
      "[10200]\tvalid's rmse: 0.0643893\ttrain's rmse: 0.0205362\n",
      "Early stopping, best iteration is:\n",
      "[9410]\tvalid's rmse: 0.0643483\ttrain's rmse: 0.0219899\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[600]\tvalid's rmse: 0.186706\ttrain's rmse: 0.196254\n",
      "[1200]\tvalid's rmse: 0.0619364\ttrain's rmse: 0.0706245\n",
      "[1800]\tvalid's rmse: 0.0595203\ttrain's rmse: 0.0594431\n",
      "[2400]\tvalid's rmse: 0.0587091\ttrain's rmse: 0.0531607\n",
      "[3000]\tvalid's rmse: 0.0579848\ttrain's rmse: 0.0481445\n",
      "[3600]\tvalid's rmse: 0.057364\ttrain's rmse: 0.044014\n",
      "[4200]\tvalid's rmse: 0.0569587\ttrain's rmse: 0.0405303\n",
      "[4800]\tvalid's rmse: 0.0565677\ttrain's rmse: 0.0375092\n",
      "[5400]\tvalid's rmse: 0.0562354\ttrain's rmse: 0.0348781\n",
      "[6000]\tvalid's rmse: 0.0559446\ttrain's rmse: 0.0325105\n",
      "[6600]\tvalid's rmse: 0.0556397\ttrain's rmse: 0.030388\n",
      "[7200]\tvalid's rmse: 0.0553673\ttrain's rmse: 0.0285111\n",
      "[7800]\tvalid's rmse: 0.0551671\ttrain's rmse: 0.0267996\n",
      "[8400]\tvalid's rmse: 0.0549796\ttrain's rmse: 0.0252675\n",
      "[9000]\tvalid's rmse: 0.0548689\ttrain's rmse: 0.0238611\n",
      "[9600]\tvalid's rmse: 0.054777\ttrain's rmse: 0.0225941\n",
      "[10200]\tvalid's rmse: 0.0546933\ttrain's rmse: 0.021417\n",
      "[10800]\tvalid's rmse: 0.054614\ttrain's rmse: 0.020325\n",
      "[11400]\tvalid's rmse: 0.0545621\ttrain's rmse: 0.0193158\n",
      "[12000]\tvalid's rmse: 0.0545124\ttrain's rmse: 0.0183968\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid's rmse: 0.0545124\ttrain's rmse: 0.0183968\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[600]\tvalid's rmse: 0.220977\ttrain's rmse: 0.18551\n",
      "[1200]\tvalid's rmse: 0.138006\ttrain's rmse: 0.0487931\n",
      "[1800]\tvalid's rmse: 0.135142\ttrain's rmse: 0.0400777\n",
      "[2400]\tvalid's rmse: 0.134616\ttrain's rmse: 0.0358127\n",
      "[3000]\tvalid's rmse: 0.13409\ttrain's rmse: 0.0326662\n",
      "[3600]\tvalid's rmse: 0.133546\ttrain's rmse: 0.0300435\n",
      "[4200]\tvalid's rmse: 0.132994\ttrain's rmse: 0.0277898\n",
      "[4800]\tvalid's rmse: 0.132527\ttrain's rmse: 0.0258534\n",
      "[5400]\tvalid's rmse: 0.132162\ttrain's rmse: 0.0241992\n",
      "[6000]\tvalid's rmse: 0.131836\ttrain's rmse: 0.0227398\n",
      "[6600]\tvalid's rmse: 0.131566\ttrain's rmse: 0.0214314\n",
      "[7200]\tvalid's rmse: 0.131299\ttrain's rmse: 0.0202832\n",
      "[7800]\tvalid's rmse: 0.131057\ttrain's rmse: 0.019154\n",
      "[8400]\tvalid's rmse: 0.130793\ttrain's rmse: 0.0180756\n",
      "[9000]\tvalid's rmse: 0.130584\ttrain's rmse: 0.0171183\n",
      "[9600]\tvalid's rmse: 0.130401\ttrain's rmse: 0.0162552\n",
      "[10200]\tvalid's rmse: 0.130194\ttrain's rmse: 0.0154335\n",
      "[10800]\tvalid's rmse: 0.130073\ttrain's rmse: 0.0146939\n",
      "[11400]\tvalid's rmse: 0.12992\ttrain's rmse: 0.0139961\n",
      "[12000]\tvalid's rmse: 0.129824\ttrain's rmse: 0.0133878\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid's rmse: 0.129824\ttrain's rmse: 0.0133878\n",
      "Baseline metrics\n",
      "      fold     train\n",
      "0        0  0.018330\n",
      "1        1  0.021990\n",
      "2        2  0.018397\n",
      "3        3  0.013388\n",
      "4  overall  0.018026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "\n",
    "app_train = pd.read_csv('xgb2lgb_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_test = pd.read_csv('xgb2lgb_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "app_train=app_train.drop(['peak_value'],axis=1)\n",
    "app_test=app_test.drop(['peak_value'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "app_train['理论输出']=app_train['光照强度']*app_train['转换效率']\n",
    "app_test['理论输出']=app_test['光照强度']*app_test['转换效率']\n",
    "\n",
    "app_train['温差']=app_train['板温']-app_train['现场温度']\n",
    "app_test['温差']=app_test['板温']-app_test['现场温度']\n",
    "\n",
    "app_train['实际功率']=app_train['转换效率']*app_train['平均功率']\n",
    "app_test['实际功率']=app_test['转换效率']*app_test['平均功率']\n",
    "\n",
    "#app_train['风力X风向']=app_train['风向']*app_train['风速']\n",
    "#app_test['风力X风向']=app_test['风向']*app_test['风速']\n",
    "\n",
    "app_train['实际温度']=app_train['转换效率']*app_train['现场温度']\n",
    "app_test['实际温度']=app_test['转换效率']*app_test['现场温度']\n",
    "\n",
    "app_train['电压差A']=app_train['电流A']-app_train['电流B']\n",
    "app_test['电压差A']=app_test['电流A']-app_test['电流B']\n",
    "\n",
    "app_train['cde']=app_train['电压A']/app_train['转换效率A']\n",
    "app_test['cde']=app_test['电压A']/app_test['转换效率A']\n",
    "app_train['cde1']=app_train['电压B']/app_train['转换效率B']\n",
    "app_test['cde1']=app_test['电压B']/app_test['转换效率B']\n",
    "app_train['cde2']=app_train['电压C']/app_train['转换效率C']\n",
    "app_test['cde2']=app_test['电压C']/app_test['转换效率C']\n",
    "\n",
    "#app_train['abk']=app_train['abk']*app_train['abk']\n",
    "#app_test['abk']=app_test['abk']*app_test['abk']\n",
    "#app_train['iuo']=app_train['光照强度']*np.cos((app_train['ID']))\n",
    "#app_test['iuo']=app_test['光照强度']*np.cos((app_test['ID']))\n",
    "app_train['cdex']=app_train['cde']*app_train['cde']\n",
    "app_test['cdex']=app_test['cde']*app_test['cde']\n",
    "app_train['cdex1']=app_train['cde1']*app_train['cde1']\n",
    "app_test['cdex1']=app_test['cde1']*app_test['cde1']\n",
    "app_train['cdex2']=app_train['cde2']*app_train['cde2']\n",
    "app_test['cdex2']=app_test['cde2']*app_test['cde2']\n",
    "\n",
    "# print(app_train['dis2peak'])\n",
    "app_train['C_1']=app_train['dis2peak']*app_train['dis2peak']\n",
    "app_test['C_1']=app_test['dis2peak']*app_test['dis2peak']\n",
    "\n",
    "app_train['C_2']=app_train['dis2peak']*app_train['光照强度']\n",
    "app_test['C_2']=app_test['dis2peak']*app_test['光照强度']\n",
    "\n",
    "app_train['C_3']=app_train['电流B']-app_train['电流C']\n",
    "app_test['C_3']=app_test['电流B']-app_test['电流C']\n",
    "\n",
    "app_train['C_4']=app_train['功率A']/app_train['风速']\n",
    "app_test['C_4']=app_test['功率A']/app_test['风速']\n",
    "\n",
    "app_train['C_5']=app_train['功率B']/app_train['风速']\n",
    "app_test['C_5']=app_test['功率B']/app_test['风速']\n",
    "\n",
    "app_train['C_6']=app_train['C_4']*app_train['C_4']\n",
    "app_test['C_6']=app_test['C_4']*app_test['C_4']\n",
    "\n",
    "app_train['C_7']=app_train['电压A']/app_train['风速']\n",
    "app_test['C_7']=app_test['电压A']/app_test['风速']\n",
    "\n",
    "app_train['C_8']=app_train['电流A']/app_train['风速']\n",
    "app_test['C_8']=app_test['电流A']/app_test['风速']\n",
    "#\n",
    "app_train['C_9']=app_train['风向']*app_train['转换效率A']\n",
    "app_test['C_9']=app_test['风向']*app_test['转换效率A']\n",
    "#\n",
    "# app_train['C_11']=app_train['板温']/app_train['现场温度']\n",
    "# app_test['C_11']=app_test['板温']/app_test['现场温度']\n",
    "#\n",
    "# app_train['C_10']=app_train['功率A']*app_train['光照强度']\n",
    "# app_test['C_10']=app_test['功率A']*app_test['光照强度']\n",
    "\n",
    "poly_features = app_train[['板温','现场温度','光照强度','风速','风向']]\n",
    "poly_features_test = app_test[['板温','现场温度','光照强度','风速','风向']]\n",
    "\n",
    "\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = app_train['发电量']\n",
    "\n",
    "\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "#print(poly_corrs)\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "''''''\n",
    "\n",
    "\n",
    "''''''\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['ID'] = app_train['ID']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'ID', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['ID'] = app_test['ID']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'ID', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_poly['发电量']=poly_target\n",
    "app_train=app_train_poly\n",
    "app_test=app_test_poly\n",
    "\n",
    "\n",
    "#app_train.to_csv('xgb_poly_timetrain.csv',index=False)\n",
    "#app_test.to_csv('xgb_poly_timetest.csv',index=False)\n",
    "\n",
    "print('Training data with polynomial features shape: ', poly_features.shape)\n",
    "print('Testing data with polynomial features shape:  ', poly_features_test.shape)\n",
    "print('Training data with polynomial features shape: ', app_train.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test.shape)\n",
    "\n",
    "\n",
    "app_train['ID']=train_id\n",
    "app_test['ID']=test_id\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 4):\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['ID']\n",
    "    test_ids = test_features['ID']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['发电量']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['ID', '发电量'])\n",
    "    test_features = test_features.drop(columns = ['ID'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    train_predictions = np.zeros(features.shape[0])\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = lgb.LGBMRegressor(objective = 'regression',n_estimators=12000,min_child_samples=20,num_leaves=26,\n",
    "                                   learning_rate = 0.005, feature_fraction=0.6,\n",
    "                                   subsample = 0.4, n_jobs = -1, random_state = 50)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'rmse',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 1000, verbose = 600)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        train_predictions += model.predict(features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict(valid_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['rmse']\n",
    "        train_score = model.best_score_['train']['rmse']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'ID': test_ids, '发电量': test_predictions})\n",
    "    train_sub = pd.DataFrame({'ID': train_ids, '发电量': train_predictions})\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    #valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    #valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metric = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            })\n",
    "\n",
    "    return submission, feature_importances, metric,train_sub\n",
    "submission, fi, metric,train_sub = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metric)\n",
    "submission.to_csv('lgb_final_result.csv', index = False)    ##  0.080844   0.84669334000\n",
    "                                                         \n",
    "                                                               # 0.012138    0.85068184000\n",
    "                                                               # 0.012227    0.85057590000\n",
    "                                                               #0.009218     0.85061880000\n",
    "                                                               # 0.010561    0.85058580000\n",
    "                                                               #0.011587     0.85059136000\n",
    "                                                               # 0.011767      850668\n",
    "                                                               #0.011500     0.85065\n",
    "                                                               #0.012576    0.85063404000\n",
    "                                                               # 0.011863  850617\n",
    "                                                               #0.012066    0.85058500000\n",
    "                                                               #0.012996    0.85058284000\n",
    "                                                               #0.0110560.85061100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据...\n",
      "['原始训练数据：', (9000, 21)]\n",
      "['测试数据：', (8409, 20)]\n",
      "合并后的数据： (17409, 20)\n",
      "正在处理的特征： ID\n",
      "正在处理的特征： 板温\n",
      "正在处理的特征： 现场温度\n",
      "正在处理的特征： 光照强度\n",
      "正在处理的特征： 转换效率\n",
      "正在处理的特征： 转换效率A\n",
      "正在处理的特征： 转换效率B\n",
      "正在处理的特征： 转换效率C\n",
      "正在处理的特征： 电压A\n",
      "正在处理的特征： 电压B\n",
      "正在处理的特征： 电压C\n",
      "正在处理的特征： 电流A\n",
      "正在处理的特征： 电流B\n",
      "正在处理的特征： 电流C\n",
      "正在处理的特征： 功率A\n",
      "正在处理的特征： 功率B\n",
      "正在处理的特征： 功率C\n",
      "正在处理的特征： 平均功率\n",
      "正在处理的特征： 风速\n",
      "正在处理的特征： 风向\n",
      "数据预处理完毕\n"
     ]
    }
   ],
   "source": [
    "# %load my_preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "#import load_data\n",
    "import pandas as pd\n",
    "\n",
    "def data_corection(data):\n",
    "\n",
    "    ID=data['ID']\n",
    "\n",
    "    for feature_num, feature in enumerate(data.keys()):\n",
    "\n",
    "        print('正在处理的特征：',feature)\n",
    "\n",
    "        Q1 = np.percentile(data[feature],25)\n",
    "        Q3 = np.percentile(data[feature],75)\n",
    "        step = (Q3-Q1)\n",
    "        if feature in ['平均功率','功率A','功率B','功率C','former_power','peak_value','I_eta']:\n",
    "            step=step*1.5\n",
    "        elif feature in ['电压A','电压B','电压C']:\n",
    "            step=step*100\n",
    "        elif feature in ['光照强度','现场温度','板温']:\n",
    "            step=step*1.5\n",
    "        elif feature in ['电流A','电流B','电流C']:\n",
    "            step=step*3\n",
    "        elif feature in ['转换效率A','转换效率B','转换效率C','转换效率']:\n",
    "            step=step*10\n",
    "        elif feature in ['风向']:\n",
    "            step=step*1.4\n",
    "        elif feature in ['风速']:\n",
    "            step=step*4\n",
    "        else:\n",
    "            step=step*1000\n",
    "\n",
    "        feature_index=data[~((data[feature] >= (Q1 - step)) & (data[feature] <= (Q3 + step)))].index\n",
    "\n",
    "        for i in range(len(feature_index)):\n",
    "            if feature_index[i]==0:\n",
    "                j=feature_index[i]+1\n",
    "                while j in feature_index:\n",
    "                    j=j+1\n",
    "                data.iloc[feature_index[i],feature_num]=data.iloc[j,feature_num]\n",
    "            else:\n",
    "                j=feature_index[i]-1\n",
    "                while j in feature_index:\n",
    "                    j=j-1\n",
    "                data.iloc[feature_index[i],feature_num]=data.iloc[j,feature_num]\n",
    "    return data\n",
    "\n",
    "def scale(features,scaler):\n",
    "\n",
    "    scaler.fit(features)\n",
    "    scaled_features=scaler.transform(features)\n",
    "    return scaled_features,scaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    print('加载数据...')\n",
    "    train_data=pd.read_csv('public.train.csv')\n",
    "\n",
    "    print(['原始训练数据：',train_data.shape])\n",
    "    test_data=pd.read_csv('public.test.csv')\n",
    "    print(['测试数据：',test_data.shape])\n",
    "\n",
    "    data=test_data.append(train_data.drop('发电量',axis=1))\n",
    "    data.sort_values('ID',inplace=True)\n",
    "    data=data.reset_index(drop=True)\n",
    "    print('合并后的数据：',data.shape)\n",
    "\n",
    "    data_prc=data_corection(data.copy())\n",
    "\n",
    "    data_prc.to_csv('data_prc_lstm.csv',index=None)\n",
    "    print('数据预处理完毕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-23 12:30:38.139227\n",
      "添加前一时刻平均功率...\n",
      "添加与平均功率峰的值距离和平均功率峰值大小...\n",
      "添加平均功率的均值和方差...\n",
      "添加平均板温...\n",
      "添加风速和风向的乘积...\n",
      "添加光照强度和转化效率的乘积\n",
      "添加功率乘以效率\n",
      "添加温差\n",
      "添加电流除以转换效率\n",
      "添加电压除以转换效率，然后平方\n",
      "添加电压除以转换效率，然后平方\n",
      "添加电流除以转换效率，然后平方\n",
      "添加温差乘以光强\n",
      "添加PN结电流\n",
      "添加功率除以风速\n",
      "添加PolyFeatures\n",
      "正在保存特征...\n",
      "特征构造完毕!总特征数量为： 89\n",
      "正在保存新的训练集和测试集...\n",
      "正在保存新的训练集和测试集...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def add_former_power(features):\n",
    "\n",
    "    mean_power=features['平均功率']\n",
    "    former_power=np.zeros(features.shape[0])\n",
    "    for i in range(1,features.shape[0]):\n",
    "        former_power[i]=mean_power[i-1]\n",
    "    former_power[0]=mean_power[0]\n",
    "\n",
    "    features.insert(1,'former_power',former_power)\n",
    "    # features.to_csv('data/features_former_power.csv',index=None)\n",
    "\n",
    "    return features\n",
    "\n",
    "def add_dis2peak(data):\n",
    "    peaks_IDs_unchecked = [90, 309, 466, 686, 844, 1046, 1209, 1402, 1597, 1775, 1976, 2163, 2349, 2540,\n",
    "             2723, 2937, 3104, 3263, 3440, 3646, 3844, 3985, 4178, 4357, 4547, 4745, 4915,\n",
    "             5175, 5387, 5607, 5803, 6018, 6229, 6443, 6635, 6816, 7017, 7206, 7432, 7633,\n",
    "             7846, 8018, 8226, 8396, 8589, 8806, 8998, 9169, 9390, 9631, 9807, 10018, 10250,\n",
    "             10432, 10645, 10825, 10998, 11230, 11428, 11632, 11850, 12054, 12264, 12476,\n",
    "             12689, 12904, 13102, 13312, 13544, 13708, 13915, 14125, 14317, 14555, 14759,\n",
    "             14952, 15173, 15399, 15612, 15813, 16036, 16240, 16436, 16651, 16838, 17028,\n",
    "             17223, 17399, 17645, 17840]\n",
    "\n",
    "    peak_IDs = []\n",
    "    peak_index=[]\n",
    "    ID=data['ID']\n",
    "    for ID_i in peaks_IDs_unchecked:\n",
    "        for j, ID_j in enumerate(ID.values):\n",
    "            if ID_i >= ID_j and ID_i < ID[j+1]:\n",
    "                peak_IDs.append(ID_j)\n",
    "                peak_index.append(j)\n",
    "\n",
    "\n",
    "    dis2peak = []\n",
    "    peak_value=[]\n",
    "    mean_power=data['平均功率']\n",
    "    for id in data['ID']:\n",
    "        mindis = np.abs(id - peak_IDs[0])\n",
    "        peak_row=peak_index[0]\n",
    "        for i,peak_id in enumerate(peak_IDs):\n",
    "            if np.abs(id-peak_id)<mindis:\n",
    "                mindis=np.abs(id-peak_id)\n",
    "                peak_row=peak_index[i]\n",
    "        dis2peak.append(mindis)\n",
    "        peak_value.append(mean_power[peak_row])\n",
    "\n",
    "    data.insert(1, 'dis2peak', dis2peak )\n",
    "    data.insert(1, 'peak_value', peak_value )\n",
    "    return data\n",
    "\n",
    "def add_power_mean_std(data):\n",
    "\n",
    "    mean_power=[]\n",
    "    std_power=[]\n",
    "\n",
    "    for dis in enumerate(data['dis2peak']):\n",
    "        mean_power_i=np.mean(data[data['dis2peak']==dis[1]]['平均功率'])\n",
    "        mean_power.append(mean_power_i)\n",
    "        std_power_i=np.std(data[data['dis2peak']==dis[1]]['平均功率'])\n",
    "        std_power.append(std_power_i)\n",
    "\n",
    "    data.insert(17,'mean_power',mean_power)\n",
    "    data.insert(17,'std_power',std_power)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_mean_board_temperature(data,T=210):\n",
    "\n",
    "    board_temperature=data['板温']\n",
    "    mean_board_temperature=[]\n",
    "\n",
    "    for i in range(len(board_temperature)):\n",
    "        temperature_i=[]\n",
    "        if i<len(board_temperature)-T:\n",
    "            temperature_i=np.sum(board_temperature[i:i+T])*1.0/T\n",
    "        else:\n",
    "            temperature_i=np.sum(board_temperature[i-T:i])*1.0/T\n",
    "        mean_board_temperature.append(temperature_i)\n",
    "    data.insert(1,'mean_board_temp',mean_board_temperature)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_wind(data,T=20):\n",
    "\n",
    "    wind0=list(data['风速']*data['风向'])\n",
    "    wind=[]\n",
    "    for i in range(len(data['风向'])):\n",
    "        wind_i=[]\n",
    "        if i<T/2:\n",
    "            wind_i=np.sum(wind0[i:i+T])*1.0/T\n",
    "        elif i<len(data['风向'])-T/2:\n",
    "            wind_i=np.sum(wind0[i-int(T/2):i+int(T/2)])*1.0/T\n",
    "        else:\n",
    "            wind_i=np.sum(wind0[i-T:i])*1.0/T\n",
    "        wind.append(wind_i)\n",
    "    data.insert(17,'wind',wind0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_I_eta(data):\n",
    "\n",
    "    data.insert(6,'I_eta',data['光照强度']*data['转换效率'])\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_P_eta(data):\n",
    "    data.insert(16,'P_eta',data['转换效率']*data['平均功率'])\n",
    "    return data\n",
    "\n",
    "def add_temp_diff(data):\n",
    "    data.insert(10,'temp_diff',data['板温']-data['现场温度'])\n",
    "    return data\n",
    "\n",
    "def add_idc(data):\n",
    "    data.insert(11,'idc_A',data['电流A']/(data['转换效率A']+0.001))\n",
    "    data.insert(11,'idc_B',data['电流B']/(data['转换效率B']+0.001))\n",
    "    data.insert(11,'idc_C',data['电流C']/(data['转换效率C']+0.001))\n",
    "    return data\n",
    "\n",
    "def add_vdc(data):\n",
    "    data.insert(11,'vdc_A',data['电压A']/(data['转换效率A']+0.001))\n",
    "    data.insert(11,'vdc_B',data['电压B']/(data['转换效率B']+0.001))\n",
    "    data.insert(11,'vdc_C',data['电压C']/(data['转换效率C']+0.001))\n",
    "    return data\n",
    "\n",
    "def add_temp_diff_light(data):\n",
    "    data.insert(10,'温差乘以光强',np.abs((data['板温']-data['现场温度'])*data['光照强度']))\n",
    "    return data\n",
    "\n",
    "def add_PN_I(data):\n",
    "    I0=4.215*10**(-14)*np.exp(0.1539*data['板温'])\n",
    "    q=1.6*10**(-9)\n",
    "    K=0.86*10**(-4)\n",
    "    U=data['电压A']\n",
    "    T=data['板温']+273.15\n",
    "    Id=I0*(np.exp((q*U)/(K*T))-1)\n",
    "    data.insert(13,'PN_I',Id*10**16.5 )\n",
    "    return data\n",
    "\n",
    "def add_vdc_square(data):\n",
    "    data.insert(14,'vdc_A_square',(data['电压A']/(data['转换效率A']+0.1))**2)\n",
    "    data.insert(14,'vdc_B_square',(data['电压B']/(data['转换效率B']+0.1))**2)\n",
    "    data.insert(14,'vdc_C_square',(data['电压C']/(data['转换效率C']+0.1))**2)\n",
    "    return data\n",
    "\n",
    "def add_idc_square(data):\n",
    "    data.insert(17,'idc_A_square',(data['电流A']/(data['转换效率A']+0.1))**2)\n",
    "    data.insert(17,'idc_B_square',(data['电流B']/(data['转换效率B']+0.1))**2)\n",
    "    data.insert(17,'idc_C_square',(data['电流C']/(data['转换效率C']+0.1))**2)\n",
    "    return data\n",
    "\n",
    "def add_power_divide_speed(data):\n",
    "    data.insert(14,'power_divide_speed_A',data['功率A']/(data['风速']+0.001))\n",
    "    data.insert(14,'power_divide_speed_B',data['功率B']/(data['风速']+0.001))\n",
    "    data.insert(14,'power_divide_speed_C',data['功率C']/(data['风速']+0.001))\n",
    "    data.insert(14,'power_divide_speed_A2',(data['功率A']/(data['风速']+0.001))**2)\n",
    "    data.insert(14,'Ic_Ib',data['电流B']-data['电流C'])\n",
    "    return data\n",
    "\n",
    "def add_poly_features(data,column_names):\n",
    "    features=data[column_names]\n",
    "    rest_features=data.drop(column_names,axis=1)\n",
    "    poly_transformer=PolynomialFeatures(degree=2,interaction_only=False,include_bias=False)\n",
    "    poly_features=pd.DataFrame(poly_transformer.fit_transform(features),columns=poly_transformer.get_feature_names(column_names))\n",
    "\n",
    "    for col in poly_features.columns:\n",
    "        rest_features.insert(1,col,poly_features[col])\n",
    "\n",
    "    return rest_features\n",
    "\n",
    "def do_feature_project(features):\n",
    "\n",
    "    print('添加前一时刻平均功率...')\n",
    "    features = add_former_power(features)\n",
    "\n",
    "    print('添加与平均功率峰的值距离和平均功率峰值大小...')\n",
    "    features=add_dis2peak(features)\n",
    "\n",
    "    print('添加平均功率的均值和方差...')\n",
    "    features=add_power_mean_std(features)\n",
    "\n",
    "    print('添加平均板温...')\n",
    "    features=add_mean_board_temperature(features)\n",
    "\n",
    "    print('添加风速和风向的乘积...')\n",
    "    features=add_wind(features,T=20)\n",
    "\n",
    "    print('添加光照强度和转化效率的乘积')\n",
    "    features=add_I_eta(features)\n",
    "\n",
    "    print('添加功率乘以效率')\n",
    "    features=add_P_eta(features)\n",
    "\n",
    "    print('添加温差')\n",
    "    features=add_temp_diff(features)\n",
    "\n",
    "    print('添加电流除以转换效率')\n",
    "    features=add_idc(features)\n",
    "\n",
    "    print('添加电压除以转换效率，然后平方')\n",
    "    features=add_vdc(features)\n",
    "\n",
    "    print('添加电压除以转换效率，然后平方')\n",
    "    features=add_vdc_square(features)\n",
    "\n",
    "    print('添加电流除以转换效率，然后平方')\n",
    "    features=add_idc_square(features)\n",
    "\n",
    "    print('添加温差乘以光强')\n",
    "    features=add_temp_diff_light(features)\n",
    "\n",
    "    print('添加PN结电流')\n",
    "    features=add_PN_I(features)\n",
    "\n",
    "    print('添加功率除以风速')\n",
    "    # features=add_power_divide_speed(features)\n",
    "\n",
    "\n",
    "    print('添加PolyFeatures')\n",
    "    # column_names=['板温','光照强度','转换效率A','电压A','电流A','风速','风向','temp_diff']\n",
    "    column_names=['板温','光照强度','转换效率A','电压A','电流A','风速','风向','temp_diff','平均功率']\n",
    "    features=add_poly_features(features,column_names)\n",
    "\n",
    "    # features=features.drop([ '电压A 电流A', '光照强度 转换效率'],axis=1)\n",
    "\n",
    "    print('正在保存特征...')\n",
    "    features.to_csv('features_lstm.csv',index=None)\n",
    "\n",
    "    print('特征构造完毕!总特征数量为：',features.shape[1])\n",
    "\n",
    "    return features\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    t0=datetime.datetime.now()\n",
    "    print(t0)\n",
    "    train_data=pd.read_csv('public.train.csv')\n",
    "    train_ID=train_data['ID']\n",
    "    test_data=pd.read_csv('public.test.csv')\n",
    "    test_ID=test_data['ID']\n",
    "\n",
    "    features=pd.read_csv('data_prc_lstm.csv')\n",
    "    # print(features.head(10))\n",
    "\n",
    "    features=do_feature_project(features)\n",
    "    # features=pd.read_csv('data/features.csv')\n",
    "\n",
    "    train_features=features[features['ID'].isin(train_ID)]\n",
    "    train_features.reset_index(drop=True)\n",
    "    train_features.insert(train_features.shape[1],'发电量',train_data['发电量'].values)\n",
    "    train_data=train_features\n",
    "\n",
    "    test_data=features[features['ID'].isin(test_ID)]\n",
    "    test_data.reset_index(drop=True)\n",
    "\n",
    "    print('正在保存新的训练集和测试集...')\n",
    "    train_data.to_csv('train_all_features_lstm.csv',index=None)\n",
    "    test_data.to_csv('test_all_features_lstm.csv',index=None)\n",
    "    print('正在保存新的训练集和测试集...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用LSTM进行回归预测\n",
      "score: 0.880517428678049\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,LSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaling function\n",
    "def scale(train_features,test_features):\n",
    "\n",
    "    scaler=MinMaxScaler()\n",
    "    data=pd.concat([train_features,test_features])\n",
    "    scaled_data=pd.DataFrame(scaler.fit_transform(data),columns=test_features.keys())\n",
    "    train_features=scaled_data.iloc[0:train_features.shape[0]]\n",
    "    test_features=scaled_data.iloc[train_features.shape[0]:]\n",
    "\n",
    "    return train_features,test_features\n",
    "\n",
    "# result saving function\n",
    "def save_predictions(IDs,predictions,name='result.csv'):\n",
    "    predictions=pd.DataFrame(list(zip(map(int,IDs),predictions)))\n",
    "    predictions.to_csv(name,header=False,index=False,sep=',')\n",
    "\n",
    "def do_LSTM_Regression():\n",
    "\n",
    "    print(\"使用LSTM进行回归预测\")\n",
    "\n",
    "\n",
    "    # load precessed data\n",
    "    train_data=pd.read_csv('train_all_features_lstm.csv')\n",
    "    train_features=train_data.drop('发电量',axis=1)\n",
    "    y=train_data['发电量']\n",
    "    X_test=pd.read_csv('test_all_features_lstm.csv')\n",
    "\n",
    "\n",
    "    # select best features for LSTM\n",
    "    selected_features=['平均功率', '电流A^2', 'ID', 'former_power', '板温 风向', '风速 风向', '电压A temp_diff',\n",
    "                       '光照强度^2', '电压A 风向', '电压A 风速', '现场温度', 'vdc_A', 'vdc_B', '电压A^2', '功率C',\n",
    "                       '功率A', '板温 电压A', '电流A', '电压A', '光照强度', 'vdc_B_square', 'vdc_A_square', 'wind',\n",
    "                       '转换效率A temp_diff', 'P_eta', '板温 temp_diff', '风向^2', '风速 temp_diff', '转换效率C',\n",
    "                       '风向', 'std_power', '板温 风速', '板温', '板温 光照强度', '板温^2', '电压B', '转换效率A 风速',\n",
    "                       '转换效率A 电流A', '板温 电流A', '电流A 风速', 'vdc_C', '转换效率A 电压A', 'vdc_C_square',\n",
    "                       'mean_board_temp', '转换效率', 'idc_B_square', 'idc_B', '功率B', '电流B', '电压A 电流A',\n",
    "                       'PN_I', '光照强度 风速', 'idc_C_square', '电压C', 'idc_C', 'I_eta', '电流C', '转换效率A^2',\n",
    "                       '转换效率A', 'idc_A_square', 'dis2peak', 'idc_A', '转换效率A 风向', '光照强度 风向',\n",
    "                       '风向 temp_diff', '风速^2', '风速', '板温 转换效率A']\n",
    "\n",
    "    #adapt the data form to LSTM\n",
    "    X_test=X_test[selected_features]\n",
    "    X_train=train_features[selected_features]\n",
    "\n",
    "    X_train,X_test=scale(X_train,X_test)# scaling\n",
    "\n",
    "    X_train=X_train.values.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "    X_test=X_test.values.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "\n",
    "    #split the data\n",
    "    X_train,X_val,y_train,y_val=train_test_split(X_train,y,test_size=0.2,random_state=333)\n",
    "\n",
    "\n",
    "    #construct LSTM based on keras\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200,activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1,activation='relu'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    #model.fit(X_train, y_train, epochs=1100, batch_size=100, validation_data=(X_val, y_val), verbose=2, shuffle=False)\n",
    "    model=load_model('lstm_model_8805_a8485.h5')# load the history best model to recovery the best result\n",
    "    #prediction\n",
    "    y_val_pred=model.predict(X_val)\n",
    "    y_test_pred=model.predict(X_test)\n",
    "\n",
    "    rmse=mean_squared_error(y_val,y_val_pred)**0.5\n",
    "    score=1.0/(1.0+rmse)\n",
    "\n",
    "    y_test_pred=np.array(y_test_pred).flatten()\n",
    "    ID=pd.read_csv('public.test.csv')['ID']\n",
    "    save_predictions(ID,y_test_pred,'result_lstm.csv')\n",
    "    print('score:',score)\n",
    "\n",
    "\n",
    "# module test\n",
    "if __name__=='__main__':\n",
    "    do_LSTM_Regression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (9000, 26)\n",
      "Testing data shape:  (8409, 26)\n",
      "Polynomial Features shape:  (9000, 21)\n",
      "Training data with polynomial features shape:  (9000, 23)\n",
      "Testing data with polynomial features shape:   (8409, 22)\n",
      "Training data with polynomial features shape:  (9000, 63)\n",
      "Testing data with polynomial features shape:   (8409, 62)\n",
      "Training Data Shape:  (9000, 61)\n",
      "Testing Data Shape:  (8409, 61)\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.184332\ttrain's rmse: 0.186485\n",
      "[1200]\tvalid's rmse: 0.0526237\ttrain's rmse: 0.0563721\n",
      "[1800]\tvalid's rmse: 0.048828\ttrain's rmse: 0.047659\n",
      "[2400]\tvalid's rmse: 0.0483967\ttrain's rmse: 0.0432651\n",
      "[3000]\tvalid's rmse: 0.0480205\ttrain's rmse: 0.0397653\n",
      "[3600]\tvalid's rmse: 0.0477495\ttrain's rmse: 0.0367731\n",
      "[4200]\tvalid's rmse: 0.0474931\ttrain's rmse: 0.0341058\n",
      "[4800]\tvalid's rmse: 0.0471991\ttrain's rmse: 0.031743\n",
      "[5400]\tvalid's rmse: 0.0469728\ttrain's rmse: 0.0296853\n",
      "[6000]\tvalid's rmse: 0.0467087\ttrain's rmse: 0.0278889\n",
      "[6600]\tvalid's rmse: 0.0465896\ttrain's rmse: 0.026353\n",
      "[7200]\tvalid's rmse: 0.0464669\ttrain's rmse: 0.0249899\n",
      "[7800]\tvalid's rmse: 0.0463334\ttrain's rmse: 0.0236955\n",
      "[8400]\tvalid's rmse: 0.0462483\ttrain's rmse: 0.0225994\n",
      "[9000]\tvalid's rmse: 0.0461893\ttrain's rmse: 0.0215576\n",
      "[9600]\tvalid's rmse: 0.0461283\ttrain's rmse: 0.0206436\n",
      "[10200]\tvalid's rmse: 0.0461155\ttrain's rmse: 0.019798\n",
      "[10800]\tvalid's rmse: 0.0461089\ttrain's rmse: 0.0190256\n",
      "[11400]\tvalid's rmse: 0.0460969\ttrain's rmse: 0.0183081\n",
      "[12000]\tvalid's rmse: 0.0460967\ttrain's rmse: 0.017634\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11949]\tvalid's rmse: 0.0460933\ttrain's rmse: 0.0176923\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.188959\ttrain's rmse: 0.185318\n",
      "[1200]\tvalid's rmse: 0.0554471\ttrain's rmse: 0.055306\n",
      "[1800]\tvalid's rmse: 0.0510483\ttrain's rmse: 0.0467138\n",
      "[2400]\tvalid's rmse: 0.0503961\ttrain's rmse: 0.0422191\n",
      "[3000]\tvalid's rmse: 0.0498575\ttrain's rmse: 0.0387528\n",
      "[3600]\tvalid's rmse: 0.0494793\ttrain's rmse: 0.0359032\n",
      "[4200]\tvalid's rmse: 0.0491221\ttrain's rmse: 0.0334459\n",
      "[4800]\tvalid's rmse: 0.0488244\ttrain's rmse: 0.0312494\n",
      "[5400]\tvalid's rmse: 0.0486989\ttrain's rmse: 0.0294219\n",
      "[6000]\tvalid's rmse: 0.0486974\ttrain's rmse: 0.027845\n",
      "[6600]\tvalid's rmse: 0.048689\ttrain's rmse: 0.0264372\n",
      "[7200]\tvalid's rmse: 0.0486228\ttrain's rmse: 0.025158\n",
      "[7800]\tvalid's rmse: 0.0485704\ttrain's rmse: 0.0239218\n",
      "[8400]\tvalid's rmse: 0.0485365\ttrain's rmse: 0.0228273\n",
      "[9000]\tvalid's rmse: 0.0485195\ttrain's rmse: 0.0218336\n",
      "[9600]\tvalid's rmse: 0.0485146\ttrain's rmse: 0.0208973\n",
      "[10200]\tvalid's rmse: 0.0485024\ttrain's rmse: 0.0200371\n",
      "[10800]\tvalid's rmse: 0.0484875\ttrain's rmse: 0.0192382\n",
      "[11400]\tvalid's rmse: 0.0485208\ttrain's rmse: 0.0184795\n",
      "[12000]\tvalid's rmse: 0.0485228\ttrain's rmse: 0.0177564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10829]\tvalid's rmse: 0.0484777\ttrain's rmse: 0.0191977\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.180246\ttrain's rmse: 0.187553\n",
      "[1200]\tvalid's rmse: 0.0482073\ttrain's rmse: 0.0565374\n",
      "[1800]\tvalid's rmse: 0.0465847\ttrain's rmse: 0.0478077\n",
      "[2400]\tvalid's rmse: 0.0461071\ttrain's rmse: 0.0434791\n",
      "[3000]\tvalid's rmse: 0.0456759\ttrain's rmse: 0.0398563\n",
      "[3600]\tvalid's rmse: 0.0453973\ttrain's rmse: 0.0368767\n",
      "[4200]\tvalid's rmse: 0.0451666\ttrain's rmse: 0.0343909\n",
      "[4800]\tvalid's rmse: 0.0450436\ttrain's rmse: 0.0322341\n",
      "[5400]\tvalid's rmse: 0.0448554\ttrain's rmse: 0.0303482\n",
      "[6000]\tvalid's rmse: 0.0446802\ttrain's rmse: 0.0285974\n",
      "[6600]\tvalid's rmse: 0.0445774\ttrain's rmse: 0.0270213\n",
      "[7200]\tvalid's rmse: 0.044432\ttrain's rmse: 0.0255985\n",
      "[7800]\tvalid's rmse: 0.044364\ttrain's rmse: 0.0243499\n",
      "[8400]\tvalid's rmse: 0.0442856\ttrain's rmse: 0.0232212\n",
      "[9000]\tvalid's rmse: 0.0442334\ttrain's rmse: 0.0221825\n",
      "[9600]\tvalid's rmse: 0.0441906\ttrain's rmse: 0.0212272\n",
      "[10200]\tvalid's rmse: 0.0441785\ttrain's rmse: 0.0202754\n",
      "[10800]\tvalid's rmse: 0.0441405\ttrain's rmse: 0.0194261\n",
      "[11400]\tvalid's rmse: 0.0441189\ttrain's rmse: 0.0186428\n",
      "[12000]\tvalid's rmse: 0.0441286\ttrain's rmse: 0.0179079\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11567]\tvalid's rmse: 0.0441142\ttrain's rmse: 0.0184344\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[600]\tvalid's rmse: 0.199903\ttrain's rmse: 0.180741\n",
      "[1200]\tvalid's rmse: 0.102409\ttrain's rmse: 0.040329\n",
      "[1800]\tvalid's rmse: 0.099588\ttrain's rmse: 0.0337289\n",
      "[2400]\tvalid's rmse: 0.0994867\ttrain's rmse: 0.0309196\n",
      "[3000]\tvalid's rmse: 0.0993034\ttrain's rmse: 0.0288595\n",
      "[3600]\tvalid's rmse: 0.0990903\ttrain's rmse: 0.0270929\n",
      "[4200]\tvalid's rmse: 0.0988493\ttrain's rmse: 0.0254848\n",
      "[4800]\tvalid's rmse: 0.0986122\ttrain's rmse: 0.024059\n",
      "[5400]\tvalid's rmse: 0.0983055\ttrain's rmse: 0.022807\n",
      "[6000]\tvalid's rmse: 0.0980873\ttrain's rmse: 0.0217062\n",
      "[6600]\tvalid's rmse: 0.0978473\ttrain's rmse: 0.0206593\n",
      "[7200]\tvalid's rmse: 0.0977323\ttrain's rmse: 0.0197653\n",
      "[7800]\tvalid's rmse: 0.0975293\ttrain's rmse: 0.0188923\n",
      "[8400]\tvalid's rmse: 0.0974306\ttrain's rmse: 0.0182047\n",
      "[9000]\tvalid's rmse: 0.0973482\ttrain's rmse: 0.0174596\n",
      "[9600]\tvalid's rmse: 0.0971942\ttrain's rmse: 0.0167532\n",
      "[10200]\tvalid's rmse: 0.0970236\ttrain's rmse: 0.0160909\n",
      "[10800]\tvalid's rmse: 0.0969044\ttrain's rmse: 0.0154822\n",
      "[11400]\tvalid's rmse: 0.0968014\ttrain's rmse: 0.0148955\n",
      "[12000]\tvalid's rmse: 0.0966932\ttrain's rmse: 0.0143522\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11991]\tvalid's rmse: 0.0966926\ttrain's rmse: 0.0143611\n",
      "Baseline metrics\n",
      "      fold     train     valid\n",
      "0        0  0.017692  0.046093\n",
      "1        1  0.019198  0.048478\n",
      "2        2  0.018434  0.044114\n",
      "3        3  0.014361  0.096693\n",
      "4  overall  0.017421  0.058844\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 23 15:30:18 2018\n",
    "\n",
    "@author: pp\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "app_train = pd.read_csv('xgb2lgb_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_test = pd.read_csv('xgb2lgb_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "app_train=app_train.drop(columns='peak_value')\n",
    "app_test=app_test.drop(columns='peak_value')\n",
    "\n",
    "\n",
    "\n",
    "app_train['理论输出']=app_train['光照强度']*app_train['转换效率']\n",
    "app_test['理论输出']=app_test['光照强度']*app_test['转换效率']\n",
    "\n",
    "app_train['温差']=app_train['板温']-app_train['现场温度']\n",
    "app_test['温差']=app_test['板温']-app_test['现场温度']\n",
    "\n",
    "app_train['实际功率']=app_train['转换效率']*app_train['平均功率']\n",
    "app_test['实际功率']=app_test['转换效率']*app_test['平均功率']\n",
    "\n",
    "#app_train['风力X风向']=app_train['风向']*app_train['风速']\n",
    "#app_test['风力X风向']=app_test['风向']*app_test['风速']\n",
    "\n",
    "app_train['实际温度']=app_train['转换效率']*app_train['现场温度']\n",
    "app_test['实际温度']=app_test['转换效率']*app_test['现场温度']\n",
    "\n",
    "app_train['电压差A']=app_train['电流A']-app_train['电流B']\n",
    "app_test['电压差A']=app_test['电流A']-app_test['电流B']\n",
    "\n",
    "app_train['cde']=app_train['电压A']/app_train['转换效率A']\n",
    "app_test['cde']=app_test['电压A']/app_test['转换效率A']\n",
    "app_train['cde1']=app_train['电压B']/app_train['转换效率B']\n",
    "app_test['cde1']=app_test['电压B']/app_test['转换效率B']\n",
    "app_train['cde2']=app_train['电压C']/app_train['转换效率C']\n",
    "app_test['cde2']=app_test['电压C']/app_test['转换效率C']\n",
    "\n",
    "#app_train['abk']=app_train['abk']*app_train['abk']\n",
    "#app_test['abk']=app_test['abk']*app_test['abk']\n",
    "#app_train['iuo']=app_train['光照强度']*np.cos((app_train['ID']))\n",
    "#app_test['iuo']=app_test['光照强度']*np.cos((app_test['ID']))\n",
    "app_train['cdex']=app_train['cde']*app_train['cde']\n",
    "app_test['cdex']=app_test['cde']*app_test['cde']\n",
    "app_train['cdex1']=app_train['cde1']*app_train['cde1']\n",
    "app_test['cdex1']=app_test['cde1']*app_test['cde1']\n",
    "app_train['cdex2']=app_train['cde2']*app_train['cde2']\n",
    "app_test['cdex2']=app_test['cde2']*app_test['cde2']\n",
    "\n",
    "# print(app_train['dis2peak'])\n",
    "app_train['C_1']=app_train['dis2peak']*app_train['dis2peak']\n",
    "app_test['C_1']=app_test['dis2peak']*app_test['dis2peak']\n",
    "\n",
    "app_train['C_2']=app_train['dis2peak']*app_train['光照强度']\n",
    "app_test['C_2']=app_test['dis2peak']*app_test['光照强度']\n",
    "\n",
    "app_train['C_3']=app_train['电流B']-app_train['电流C']\n",
    "app_test['C_3']=app_test['电流B']-app_test['电流C']\n",
    "\n",
    "\n",
    "# app_train['实际温度1']=app_train['转换效率']*app_train['风向']\n",
    "# app_test['实际温度1']=app_test['转换效率']*app_test['风向']\n",
    "\n",
    "app_train['C_4']=app_train['功率A']/app_train['风速']\n",
    "app_test['C_4']=app_test['功率A']/app_test['风速']\n",
    "\n",
    "app_train['C_5']=app_train['功率B']/app_train['风速']\n",
    "app_test['C_5']=app_test['功率B']/app_test['风速']\n",
    "\n",
    "app_train['C_6']=app_train['C_4']*app_train['C_4']\n",
    "app_test['C_6']=app_test['C_4']*app_test['C_4']\n",
    "\n",
    "# app_train['C_7']=app_train['功率A']*app_train['风向']\n",
    "# app_test['C_7']=app_test['功率A']*app_test['风向']\n",
    "\n",
    "# app_train['实际温度3']=app_train['功率C']/app_train['风速']\n",
    "# app_test['实际温度3']=app_test['功率C']/app_test['风速']\n",
    "#app_train['C_9']=app_train['风向']*app_train['转换效率A']\n",
    "#app_test['C_9']=app_test['风向']*app_test['转换效率A']\n",
    "\n",
    "# app_train['实际温度3']=app_train['功率B']*app_train['风速']\n",
    "# app_test['实际温度3']=app_test['功率B']*app_test['风速']\n",
    "\n",
    "# app_train['cde5']=app_train['cde4'] *app_train['cde4']\n",
    "# app_test['cde5']=app_test['cde4'] * app_test['cde4']\n",
    "\n",
    "# app_train['C_3']=app_train['电流C']-app_train['电流A']\n",
    "# app_test['C_3']=app_test['电流C']-app_test['电流A']\n",
    "\n",
    "poly_features = app_train[['板温','现场温度','光照强度','风速','风向']]\n",
    "poly_features_test = app_test[['板温','现场温度','光照强度','风速','风向']]\n",
    "\n",
    "\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = app_train['发电量']\n",
    "\n",
    "\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "#print(poly_corrs)\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "''''''\n",
    "\n",
    "\n",
    "''''''\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['ID'] = app_train['ID']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'ID', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['ID'] = app_test['ID']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'ID', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_poly['发电量']=poly_target\n",
    "app_train=app_train_poly\n",
    "app_test=app_test_poly\n",
    "\n",
    "\n",
    "#app_train.to_csv('xgb_poly_timetrain.csv',index=False)\n",
    "#app_test.to_csv('xgb_poly_timetest.csv',index=False)\n",
    "\n",
    "print('Training data with polynomial features shape: ', poly_features.shape)\n",
    "print('Testing data with polynomial features shape:  ', poly_features_test.shape)\n",
    "print('Training data with polynomial features shape: ', app_train.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test.shape)\n",
    "\n",
    "\n",
    "app_train['ID']=train_id\n",
    "app_test['ID']=test_id\n",
    "\n",
    "#app_train = app_train.drop(['1', '板温_y','现场温度_y', '光照强度_y', '风速_y', '风向_y'],axis=1)\n",
    "#app_test = app_test.drop(['1', '板温_y','现场温度_y', '光照强度_y', '风速_y', '风向_y'],axis=1)\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 4):\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['ID']\n",
    "    test_ids = test_features['ID']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['发电量']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['ID', '发电量'])\n",
    "    test_features = test_features.drop(columns = ['ID'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    train_predictions = np.zeros(features.shape[0])\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = lgb.LGBMRegressor(objective = 'regression',n_estimators=12000,min_child_samples=20,num_leaves=20,\n",
    "                                   learning_rate = 0.005, feature_fraction=0.8,\n",
    "                                   subsample = 0.5, n_jobs = -1, random_state = 50)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'rmse',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 2000, verbose = 600)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        train_predictions += model.predict(features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict(valid_features, num_iteration = best_iteration)/ k_fold.n_splits\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['rmse']\n",
    "        train_score = model.best_score_['train']['rmse']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'ID': test_ids, '发电量': test_predictions})\n",
    "    train_sub = pd.DataFrame({'ID': train_ids, '发电量': train_predictions})\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    #valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    #valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    valid_scores.append(np.mean(valid_scores))\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metric = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid':valid_scores})\n",
    "\n",
    "    return submission, feature_importances, metric,train_sub\n",
    "submission, fi, metric,train_sub = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metric)\n",
    "lgb2xgb_train = pd.read_csv('train_all_features2.csv')\n",
    "lgb2xgb_test = pd.read_csv('test_all_features2.csv')\n",
    "lgb2xgb_train['lgb发电量']=train_sub['发电量']\n",
    "lgb2xgb_test['lgb发电量']=submission['发电量']\n",
    "lgb2xgb_train.to_csv('lgb2xgb_train.csv',index=False)\n",
    "lgb2xgb_test.to_csv('lgb2xgb_test.csv', index = False)\n",
    "#submission.to_csv('poly_time_54f_test.csv', index = False)    ##  0.080844   0.84669334000\n",
    "                                               \n",
    "                                                               ##  0.079003    0.84672240000\n",
    "                                                               #0.093215     0.8457\n",
    "                                                               #0.078988   0.84892\n",
    "                                                               #0.078988   0.84897390000\n",
    "                                                               #0.84902996000\n",
    "                                                               #0.066291  0.84906185000\n",
    "                                                               #0.065589  0.84901553000\n",
    "                                                               #0.068688  0.84901680000\n",
    "                                                               #0.065880  0.8488367000\n",
    "                                                               #0.067978  0.84887385000\n",
    "                                                               #0.063314 0.84917360000\n",
    "                                                               #0.073647  0.84889290000\n",
    "                                                               #0.065066  0.84911215000\n",
    "                                                               #0.063880  0.84924483000\n",
    "                                                               #0.065388  0.84930140000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (9000, 26)\n",
      "Testing data shape:  (8409, 26)\n",
      "Polynomial Features shape:  (9000, 21)\n",
      "Training data with polynomial features shape:  (9000, 23)\n",
      "Testing data with polynomial features shape:   (8409, 22)\n",
      "Training data with polynomial features shape:  (9000, 62)\n",
      "Testing data with polynomial features shape:   (8409, 61)\n",
      "Training Data Shape:  (9000, 60)\n",
      "Testing Data Shape:  (8409, 60)\n",
      "[0]\tvalidation_0-rmse:6.22018\tvalidation_1-rmse:6.19078\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1500 rounds.\n",
      "[600]\tvalidation_0-rmse:0.323974\tvalidation_1-rmse:0.324173\n",
      "[1200]\tvalidation_0-rmse:0.05029\tvalidation_1-rmse:0.080365\n",
      "[1800]\tvalidation_0-rmse:0.036313\tvalidation_1-rmse:0.075411\n",
      "[2400]\tvalidation_0-rmse:0.030766\tvalidation_1-rmse:0.073997\n",
      "[3000]\tvalidation_0-rmse:0.026904\tvalidation_1-rmse:0.073054\n",
      "[3600]\tvalidation_0-rmse:0.023689\tvalidation_1-rmse:0.072427\n",
      "[4200]\tvalidation_0-rmse:0.021145\tvalidation_1-rmse:0.071984\n",
      "[4800]\tvalidation_0-rmse:0.019047\tvalidation_1-rmse:0.071599\n",
      "[5400]\tvalidation_0-rmse:0.017296\tvalidation_1-rmse:0.071326\n",
      "[6000]\tvalidation_0-rmse:0.01576\tvalidation_1-rmse:0.071086\n",
      "[6600]\tvalidation_0-rmse:0.014412\tvalidation_1-rmse:0.070887\n",
      "[7200]\tvalidation_0-rmse:0.013212\tvalidation_1-rmse:0.070727\n",
      "[7800]\tvalidation_0-rmse:0.012116\tvalidation_1-rmse:0.070597\n",
      "[8400]\tvalidation_0-rmse:0.011163\tvalidation_1-rmse:0.070516\n",
      "[9000]\tvalidation_0-rmse:0.010296\tvalidation_1-rmse:0.070443\n",
      "[9600]\tvalidation_0-rmse:0.00951\tvalidation_1-rmse:0.070386\n",
      "[10200]\tvalidation_0-rmse:0.008814\tvalidation_1-rmse:0.070332\n",
      "[10800]\tvalidation_0-rmse:0.008154\tvalidation_1-rmse:0.070265\n",
      "[11400]\tvalidation_0-rmse:0.007553\tvalidation_1-rmse:0.070225\n",
      "[12000]\tvalidation_0-rmse:0.007015\tvalidation_1-rmse:0.070176\n",
      "[12600]\tvalidation_0-rmse:0.006536\tvalidation_1-rmse:0.07014\n",
      "[13200]\tvalidation_0-rmse:0.006079\tvalidation_1-rmse:0.070112\n",
      "[13800]\tvalidation_0-rmse:0.005664\tvalidation_1-rmse:0.070087\n",
      "[14400]\tvalidation_0-rmse:0.005272\tvalidation_1-rmse:0.070067\n",
      "[15000]\tvalidation_0-rmse:0.004918\tvalidation_1-rmse:0.070055\n",
      "[15600]\tvalidation_0-rmse:0.004585\tvalidation_1-rmse:0.070043\n",
      "[16200]\tvalidation_0-rmse:0.004284\tvalidation_1-rmse:0.070032\n",
      "[16800]\tvalidation_0-rmse:0.003997\tvalidation_1-rmse:0.070022\n",
      "[17400]\tvalidation_0-rmse:0.003742\tvalidation_1-rmse:0.070013\n",
      "[18000]\tvalidation_0-rmse:0.003501\tvalidation_1-rmse:0.070005\n",
      "[18600]\tvalidation_0-rmse:0.003275\tvalidation_1-rmse:0.069999\n",
      "[19200]\tvalidation_0-rmse:0.003066\tvalidation_1-rmse:0.069995\n",
      "[19800]\tvalidation_0-rmse:0.002872\tvalidation_1-rmse:0.069992\n",
      "[20400]\tvalidation_0-rmse:0.002691\tvalidation_1-rmse:0.069986\n",
      "[21000]\tvalidation_0-rmse:0.002524\tvalidation_1-rmse:0.069979\n",
      "[21600]\tvalidation_0-rmse:0.002367\tvalidation_1-rmse:0.069975\n",
      "[22200]\tvalidation_0-rmse:0.002221\tvalidation_1-rmse:0.06997\n",
      "[22800]\tvalidation_0-rmse:0.002086\tvalidation_1-rmse:0.069967\n",
      "[23400]\tvalidation_0-rmse:0.001959\tvalidation_1-rmse:0.069964\n",
      "[24000]\tvalidation_0-rmse:0.001843\tvalidation_1-rmse:0.06996\n",
      "[24600]\tvalidation_0-rmse:0.001732\tvalidation_1-rmse:0.069958\n",
      "[25200]\tvalidation_0-rmse:0.001629\tvalidation_1-rmse:0.069957\n",
      "[25800]\tvalidation_0-rmse:0.001533\tvalidation_1-rmse:0.069955\n",
      "[25999]\tvalidation_0-rmse:0.001501\tvalidation_1-rmse:0.069955\n",
      "[0]\tvalidation_0-rmse:6.20308\tvalidation_1-rmse:6.24208\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1500 rounds.\n",
      "[600]\tvalidation_0-rmse:0.323528\tvalidation_1-rmse:0.334743\n",
      "[1200]\tvalidation_0-rmse:0.050679\tvalidation_1-rmse:0.084129\n",
      "[1800]\tvalidation_0-rmse:0.037017\tvalidation_1-rmse:0.07737\n",
      "[2400]\tvalidation_0-rmse:0.031639\tvalidation_1-rmse:0.075419\n",
      "[3000]\tvalidation_0-rmse:0.027727\tvalidation_1-rmse:0.074141\n",
      "[3600]\tvalidation_0-rmse:0.024555\tvalidation_1-rmse:0.073166\n",
      "[4200]\tvalidation_0-rmse:0.021982\tvalidation_1-rmse:0.072476\n",
      "[4800]\tvalidation_0-rmse:0.019855\tvalidation_1-rmse:0.071987\n",
      "[5400]\tvalidation_0-rmse:0.018046\tvalidation_1-rmse:0.071617\n",
      "[6000]\tvalidation_0-rmse:0.016446\tvalidation_1-rmse:0.071272\n",
      "[6600]\tvalidation_0-rmse:0.015048\tvalidation_1-rmse:0.071027\n",
      "[7200]\tvalidation_0-rmse:0.01381\tvalidation_1-rmse:0.070849\n",
      "[7800]\tvalidation_0-rmse:0.012705\tvalidation_1-rmse:0.070687\n",
      "[8400]\tvalidation_0-rmse:0.01173\tvalidation_1-rmse:0.070575\n",
      "[9000]\tvalidation_0-rmse:0.010811\tvalidation_1-rmse:0.070469\n",
      "[9600]\tvalidation_0-rmse:0.009989\tvalidation_1-rmse:0.070368\n",
      "[10200]\tvalidation_0-rmse:0.009254\tvalidation_1-rmse:0.070294\n",
      "[10800]\tvalidation_0-rmse:0.008593\tvalidation_1-rmse:0.070242\n",
      "[11400]\tvalidation_0-rmse:0.007982\tvalidation_1-rmse:0.070172\n",
      "[12000]\tvalidation_0-rmse:0.007414\tvalidation_1-rmse:0.070121\n",
      "[12600]\tvalidation_0-rmse:0.006897\tvalidation_1-rmse:0.07008\n",
      "[13200]\tvalidation_0-rmse:0.006423\tvalidation_1-rmse:0.070043\n",
      "[13800]\tvalidation_0-rmse:0.005981\tvalidation_1-rmse:0.070004\n",
      "[14400]\tvalidation_0-rmse:0.005569\tvalidation_1-rmse:0.069974\n",
      "[15000]\tvalidation_0-rmse:0.00521\tvalidation_1-rmse:0.06995\n",
      "[15600]\tvalidation_0-rmse:0.004872\tvalidation_1-rmse:0.069926\n",
      "[16200]\tvalidation_0-rmse:0.004559\tvalidation_1-rmse:0.069912\n",
      "[16800]\tvalidation_0-rmse:0.004268\tvalidation_1-rmse:0.069898\n",
      "[17400]\tvalidation_0-rmse:0.003993\tvalidation_1-rmse:0.069884\n",
      "[18000]\tvalidation_0-rmse:0.003741\tvalidation_1-rmse:0.06987\n",
      "[18600]\tvalidation_0-rmse:0.003503\tvalidation_1-rmse:0.069855\n",
      "[19200]\tvalidation_0-rmse:0.003282\tvalidation_1-rmse:0.06984\n",
      "[19800]\tvalidation_0-rmse:0.00308\tvalidation_1-rmse:0.069829\n",
      "[20400]\tvalidation_0-rmse:0.002888\tvalidation_1-rmse:0.06982\n",
      "[21000]\tvalidation_0-rmse:0.002714\tvalidation_1-rmse:0.069811\n",
      "[21600]\tvalidation_0-rmse:0.002551\tvalidation_1-rmse:0.069805\n",
      "[22200]\tvalidation_0-rmse:0.002395\tvalidation_1-rmse:0.069796\n",
      "[22800]\tvalidation_0-rmse:0.002253\tvalidation_1-rmse:0.069792\n",
      "[23400]\tvalidation_0-rmse:0.002117\tvalidation_1-rmse:0.069783\n",
      "[24000]\tvalidation_0-rmse:0.001991\tvalidation_1-rmse:0.069777\n",
      "[24600]\tvalidation_0-rmse:0.001876\tvalidation_1-rmse:0.069774\n",
      "[25200]\tvalidation_0-rmse:0.001766\tvalidation_1-rmse:0.069768\n",
      "[25800]\tvalidation_0-rmse:0.001664\tvalidation_1-rmse:0.069767\n",
      "[25999]\tvalidation_0-rmse:0.001631\tvalidation_1-rmse:0.069765\n",
      "[0]\tvalidation_0-rmse:6.21716\tvalidation_1-rmse:6.19987\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1500 rounds.\n",
      "[600]\tvalidation_0-rmse:0.325071\tvalidation_1-rmse:0.325296\n",
      "[1200]\tvalidation_0-rmse:0.052228\tvalidation_1-rmse:0.072967\n",
      "[1800]\tvalidation_0-rmse:0.037818\tvalidation_1-rmse:0.067292\n",
      "[2400]\tvalidation_0-rmse:0.032382\tvalidation_1-rmse:0.065468\n",
      "[3000]\tvalidation_0-rmse:0.028398\tvalidation_1-rmse:0.064316\n",
      "[3600]\tvalidation_0-rmse:0.025266\tvalidation_1-rmse:0.0635\n",
      "[4200]\tvalidation_0-rmse:0.022708\tvalidation_1-rmse:0.062937\n",
      "[4800]\tvalidation_0-rmse:0.020459\tvalidation_1-rmse:0.062521\n",
      "[5400]\tvalidation_0-rmse:0.018606\tvalidation_1-rmse:0.062217\n",
      "[6000]\tvalidation_0-rmse:0.016958\tvalidation_1-rmse:0.061943\n",
      "[6600]\tvalidation_0-rmse:0.015518\tvalidation_1-rmse:0.061753\n",
      "[7200]\tvalidation_0-rmse:0.01422\tvalidation_1-rmse:0.061594\n",
      "[7800]\tvalidation_0-rmse:0.013076\tvalidation_1-rmse:0.061452\n",
      "[8400]\tvalidation_0-rmse:0.012044\tvalidation_1-rmse:0.061372\n",
      "[9000]\tvalidation_0-rmse:0.011117\tvalidation_1-rmse:0.061301\n",
      "[9600]\tvalidation_0-rmse:0.010277\tvalidation_1-rmse:0.061247\n",
      "[10200]\tvalidation_0-rmse:0.009493\tvalidation_1-rmse:0.061197\n",
      "[10800]\tvalidation_0-rmse:0.0088\tvalidation_1-rmse:0.061163\n",
      "[11400]\tvalidation_0-rmse:0.008153\tvalidation_1-rmse:0.061143\n",
      "[12000]\tvalidation_0-rmse:0.007576\tvalidation_1-rmse:0.06112\n",
      "[12600]\tvalidation_0-rmse:0.007037\tvalidation_1-rmse:0.061095\n",
      "[13200]\tvalidation_0-rmse:0.006556\tvalidation_1-rmse:0.061078\n",
      "[13800]\tvalidation_0-rmse:0.006103\tvalidation_1-rmse:0.061063\n",
      "[14400]\tvalidation_0-rmse:0.005682\tvalidation_1-rmse:0.061061\n",
      "[15000]\tvalidation_0-rmse:0.005298\tvalidation_1-rmse:0.061053\n",
      "[15600]\tvalidation_0-rmse:0.004943\tvalidation_1-rmse:0.061044\n",
      "[16200]\tvalidation_0-rmse:0.004616\tvalidation_1-rmse:0.061039\n",
      "[16800]\tvalidation_0-rmse:0.004314\tvalidation_1-rmse:0.061032\n",
      "[17400]\tvalidation_0-rmse:0.004033\tvalidation_1-rmse:0.061025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18000]\tvalidation_0-rmse:0.003767\tvalidation_1-rmse:0.061016\n",
      "[18600]\tvalidation_0-rmse:0.003527\tvalidation_1-rmse:0.061007\n",
      "[19200]\tvalidation_0-rmse:0.0033\tvalidation_1-rmse:0.061005\n",
      "[19800]\tvalidation_0-rmse:0.003093\tvalidation_1-rmse:0.061001\n",
      "[20400]\tvalidation_0-rmse:0.002901\tvalidation_1-rmse:0.061001\n",
      "[21000]\tvalidation_0-rmse:0.00272\tvalidation_1-rmse:0.060999\n",
      "[21600]\tvalidation_0-rmse:0.002553\tvalidation_1-rmse:0.060997\n",
      "[22200]\tvalidation_0-rmse:0.002396\tvalidation_1-rmse:0.060996\n",
      "[22800]\tvalidation_0-rmse:0.002252\tvalidation_1-rmse:0.060997\n",
      "[23400]\tvalidation_0-rmse:0.002117\tvalidation_1-rmse:0.060993\n",
      "[24000]\tvalidation_0-rmse:0.001991\tvalidation_1-rmse:0.060994\n",
      "[24600]\tvalidation_0-rmse:0.001874\tvalidation_1-rmse:0.060992\n",
      "[25200]\tvalidation_0-rmse:0.001764\tvalidation_1-rmse:0.060992\n",
      "[25800]\tvalidation_0-rmse:0.001664\tvalidation_1-rmse:0.060993\n",
      "[25999]\tvalidation_0-rmse:0.001631\tvalidation_1-rmse:0.060993\n",
      "[0]\tvalidation_0-rmse:6.21095\tvalidation_1-rmse:6.21857\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1500 rounds.\n",
      "[600]\tvalidation_0-rmse:0.320924\tvalidation_1-rmse:0.351773\n",
      "[1200]\tvalidation_0-rmse:0.044922\tvalidation_1-rmse:0.154818\n",
      "[1800]\tvalidation_0-rmse:0.032548\tvalidation_1-rmse:0.151525\n",
      "[2400]\tvalidation_0-rmse:0.027759\tvalidation_1-rmse:0.150285\n",
      "[3000]\tvalidation_0-rmse:0.024432\tvalidation_1-rmse:0.149355\n",
      "[3600]\tvalidation_0-rmse:0.021726\tvalidation_1-rmse:0.148687\n",
      "[4200]\tvalidation_0-rmse:0.019588\tvalidation_1-rmse:0.148243\n",
      "[4800]\tvalidation_0-rmse:0.017769\tvalidation_1-rmse:0.147918\n",
      "[5400]\tvalidation_0-rmse:0.016234\tvalidation_1-rmse:0.147679\n",
      "[6000]\tvalidation_0-rmse:0.014887\tvalidation_1-rmse:0.147432\n",
      "[6600]\tvalidation_0-rmse:0.013679\tvalidation_1-rmse:0.147264\n",
      "[7200]\tvalidation_0-rmse:0.012582\tvalidation_1-rmse:0.147086\n",
      "[7800]\tvalidation_0-rmse:0.011583\tvalidation_1-rmse:0.146975\n",
      "[8400]\tvalidation_0-rmse:0.01069\tvalidation_1-rmse:0.146895\n",
      "[9000]\tvalidation_0-rmse:0.009879\tvalidation_1-rmse:0.146804\n",
      "[9600]\tvalidation_0-rmse:0.009146\tvalidation_1-rmse:0.146735\n",
      "[10200]\tvalidation_0-rmse:0.008474\tvalidation_1-rmse:0.146672\n",
      "[10800]\tvalidation_0-rmse:0.007849\tvalidation_1-rmse:0.146612\n",
      "[11400]\tvalidation_0-rmse:0.007292\tvalidation_1-rmse:0.146576\n",
      "[12000]\tvalidation_0-rmse:0.006776\tvalidation_1-rmse:0.146537\n",
      "[12600]\tvalidation_0-rmse:0.006302\tvalidation_1-rmse:0.146491\n",
      "[13200]\tvalidation_0-rmse:0.005869\tvalidation_1-rmse:0.146461\n",
      "[13800]\tvalidation_0-rmse:0.005478\tvalidation_1-rmse:0.146434\n",
      "[14400]\tvalidation_0-rmse:0.005116\tvalidation_1-rmse:0.146407\n",
      "[15000]\tvalidation_0-rmse:0.004781\tvalidation_1-rmse:0.146377\n",
      "[15600]\tvalidation_0-rmse:0.004474\tvalidation_1-rmse:0.146359\n",
      "[16200]\tvalidation_0-rmse:0.004182\tvalidation_1-rmse:0.146347\n",
      "[16800]\tvalidation_0-rmse:0.003912\tvalidation_1-rmse:0.146333\n",
      "[17400]\tvalidation_0-rmse:0.003661\tvalidation_1-rmse:0.146322\n",
      "[18000]\tvalidation_0-rmse:0.003426\tvalidation_1-rmse:0.146308\n",
      "[18600]\tvalidation_0-rmse:0.003215\tvalidation_1-rmse:0.146301\n",
      "[19200]\tvalidation_0-rmse:0.003013\tvalidation_1-rmse:0.146295\n",
      "[19800]\tvalidation_0-rmse:0.002825\tvalidation_1-rmse:0.146291\n",
      "[20400]\tvalidation_0-rmse:0.002654\tvalidation_1-rmse:0.146286\n",
      "[21000]\tvalidation_0-rmse:0.002496\tvalidation_1-rmse:0.146283\n",
      "[21600]\tvalidation_0-rmse:0.002344\tvalidation_1-rmse:0.146276\n",
      "[22200]\tvalidation_0-rmse:0.002201\tvalidation_1-rmse:0.146273\n",
      "[22800]\tvalidation_0-rmse:0.00207\tvalidation_1-rmse:0.146272\n",
      "[23400]\tvalidation_0-rmse:0.00195\tvalidation_1-rmse:0.146269\n",
      "[24000]\tvalidation_0-rmse:0.001834\tvalidation_1-rmse:0.146264\n",
      "[24600]\tvalidation_0-rmse:0.001727\tvalidation_1-rmse:0.14626\n",
      "[25200]\tvalidation_0-rmse:0.001625\tvalidation_1-rmse:0.146259\n",
      "[25800]\tvalidation_0-rmse:0.001533\tvalidation_1-rmse:0.146255\n",
      "[25999]\tvalidation_0-rmse:0.001503\tvalidation_1-rmse:0.146255\n",
      "Baseline metrics\n",
      "      fold     train\n",
      "0        0  4.982509\n",
      "1        1  5.020884\n",
      "2        2  4.990708\n",
      "3        3  5.005490\n",
      "4  overall  4.999898\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "app_train = pd.read_csv('lgb2xgb_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_test = pd.read_csv('lgb2xgb_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "train_id=app_train[['ID']]\n",
    "test_id=app_test[['ID']]\n",
    "app_train_test = [app_train, app_test]\n",
    "app_train_test = pd.concat(app_train_test)\n",
    "app_train_test=app_train_test.mask(app_train_test.sub(app_train_test.mean()).div(app_train_test.std()).abs().gt(3))\n",
    "'''may change ffill'''\n",
    "app_train_test=app_train_test.fillna(method='ffill')\n",
    "app_train= train_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test= test_id.merge(app_train_test, on='ID', how='left')\n",
    "app_test=app_test.drop(columns='发电量')\n",
    "app_train=app_train.drop(columns='peak_value')\n",
    "app_test=app_test.drop(columns='peak_value')\n",
    "\n",
    "\n",
    "'''a'''\n",
    "app_train['理论输出']=app_train['光照强度']*app_train['转换效率']\n",
    "app_test['理论输出']=app_test['光照强度']*app_test['转换效率']\n",
    "'''b'''\n",
    "app_train['温差']=app_train['板温']-app_train['现场温度']\n",
    "app_test['温差']=app_test['板温']-app_test['现场温度']\n",
    "'''c'''\n",
    "app_train['实际功率']=app_train['转换效率']*app_train['平均功率']\n",
    "app_test['实际功率']=app_test['转换效率']*app_test['平均功率']\n",
    "'''d'''\n",
    "app_train['风力X风向']=app_train['风向']*app_train['风速']\n",
    "app_test['风力X风向']=app_test['风向']*app_test['风速']\n",
    "\n",
    "app_train['实际温度']=app_train['转换效率']*app_train['现场温度']\n",
    "app_test['实际温度']=app_test['转换效率']*app_test['现场温度']\n",
    "'''开始瞎jb蒙'''\n",
    "app_train['电压差A']=app_train['电流A']-app_train['电流B']\n",
    "app_test['电压差A']=app_test['电流A']-app_test['电流B']\n",
    "\n",
    "app_train['cde']=app_train['电压A']/app_train['转换效率A']\n",
    "app_test['cde']=app_test['电压A']/app_test['转换效率A']\n",
    "app_train['cde1']=app_train['电压B']/app_train['转换效率B']\n",
    "app_test['cde1']=app_test['电压B']/app_test['转换效率B']\n",
    "app_train['cde2']=app_train['电压C']/app_train['转换效率C']\n",
    "app_test['cde2']=app_test['电压C']/app_test['转换效率C']\n",
    "\n",
    "#app_train['abk']=app_train['abk']*app_train['abk']\n",
    "#app_test['abk']=app_test['abk']*app_test['abk']\n",
    "#app_train['iuo']=app_train['光照强度']*np.cos((app_train['ID']))\n",
    "#app_test['iuo']=app_test['光照强度']*np.cos((app_test['ID']))\n",
    "app_train['cdex']=app_train['cde']*app_train['cde']\n",
    "app_test['cdex']=app_test['cde']*app_test['cde']\n",
    "app_train['cdex1']=app_train['cde1']*app_train['cde1']\n",
    "app_test['cdex1']=app_test['cde1']*app_test['cde1']\n",
    "app_train['cdex2']=app_train['cde2']*app_train['cde2']\n",
    "app_test['cdex2']=app_test['cde2']*app_test['cde2']\n",
    "\n",
    "\n",
    "app_train['C_1']=app_train['dis2peak']*app_train['dis2peak']\n",
    "app_test['C_1']=app_test['dis2peak']*app_test['dis2peak']\n",
    "\n",
    "app_train['C_2']=app_train['dis2peak']*app_train['光照强度']\n",
    "app_test['C_2']=app_test['dis2peak']*app_test['光照强度']\n",
    "\n",
    "app_train['C_3']=app_train['电流B']-app_train['电流C']\n",
    "app_test['C_3']=app_test['电流B']-app_test['电流C']\n",
    "\n",
    "#app_train['C_17']=app_train['电压A']/app_train['风速']\n",
    "#app_test['C_17']=app_test['电压A']/app_test['风速']\n",
    "\n",
    "\n",
    "#app_train['C_8']=app_train['电流A']/app_train['风速']\n",
    "#app_test['C_8']=app_test['电流A']/app_test['风速']\n",
    "\n",
    "\n",
    "app_train['C_9']=app_train['风向']*app_train['转换效率A']\n",
    "app_test['C_9']=app_test['风向']*app_test['转换效率A']\n",
    "#app_train['C_4']=app_train['功率A']/app_train['风速']\n",
    "#app_test['C_4']=app_test['功率A']/app_test['风速']\n",
    "\n",
    "#app_train['C_6']=app_train['C_4']*app_train['C_4']\n",
    "#app_test['C_6']=app_test['C_4']*app_test['C_4']\n",
    "#app_train['C_5']=app_train['功率B']/app_train['风速']\n",
    "#app_test['C_5']=app_test['功率B']/app_test['风速']\n",
    "\n",
    "\n",
    "poly_features = app_train[['板温','现场温度','光照强度','风速','风向']]\n",
    "poly_features_test = app_test[['板温','现场温度','光照强度','风速','风向']]\n",
    "\n",
    "\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_target = app_train['发电量']\n",
    "\n",
    "\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "poly_features = pd.DataFrame(poly_features,\n",
    "                             columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "#print(poly_corrs)\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test,\n",
    "                                  columns = poly_transformer.get_feature_names(['板温','现场温度','光照强度','风速','风向']))\n",
    "''''''\n",
    "\n",
    "\n",
    "''''''\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['ID'] = app_train['ID']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'ID', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['ID'] = app_test['ID']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'ID', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "app_train_poly['发电量']=poly_target\n",
    "app_train=app_train_poly\n",
    "app_test=app_test_poly\n",
    "\n",
    "\n",
    "#app_train.to_csv('xgb_poly_timetrain.csv',index=False)\n",
    "#app_test.to_csv('xgb_poly_timetest.csv',index=False)\n",
    "\n",
    "print('Training data with polynomial features shape: ', poly_features.shape)\n",
    "print('Testing data with polynomial features shape:  ', poly_features_test.shape)\n",
    "print('Training data with polynomial features shape: ', app_train.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test.shape)\n",
    "\n",
    "\n",
    "app_train['ID']=train_id\n",
    "app_test['ID']=test_id\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import gc\n",
    "import math\n",
    "def my_scorer(y_true, y_predicted,X_test):\n",
    "    loss_train = np.sum((y_true - y_predicted)**2, axis=0) / (X_test.shape[0])  #RMSE\n",
    "    loss_train = loss_train **0.5\n",
    "    score = 1/(1+loss_train)\n",
    "    return loss_train\n",
    "\n",
    " \n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 4):\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['ID']\n",
    "    test_ids = test_features['ID']\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features['发电量']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['ID', '发电量'])\n",
    "    test_features = test_features.drop(columns = ['ID'])\n",
    "\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    train_predictions = np.zeros(features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = xgb.XGBRegressor(objective = 'reg:linear',n_estimators=26000,min_child_weight=1,num_leaves=28,\n",
    "                                   learning_rate = 0.005, max_depth=6,n_jobs=8,\n",
    "                                   subsample = 0.6, colsample_bytree = 0.4, colsample_bylevel = 1)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels,\n",
    "                  eval_set = [(train_features, train_labels),(valid_features, valid_labels)],\n",
    "                  early_stopping_rounds = 1500, verbose = 600)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = 16000\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict(test_features)/ k_fold.n_splits\n",
    "        train_predictions += model.predict(features)/ k_fold.n_splits\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold      = model.predict(valid_features)/ k_fold.n_splits\n",
    "\n",
    "        # Record the best score\n",
    "        train_score =  my_scorer(valid_labels,out_of_fold,valid_features)\n",
    "\n",
    "        # valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'ID': test_ids, '发电量': test_predictions})\n",
    "    train_sub = pd.DataFrame({'ID': train_ids, '发电量': train_predictions})\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    #valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    # Add the overall scores to the metrics\n",
    "    #valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metric = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            })\n",
    "\n",
    "    return submission, feature_importances, metric,train_sub\n",
    "submission, fi, metric,train_sub = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metric)\n",
    "submission.to_csv('reuslt_xgb1.csv', index = False)    ##  0.080844   0.84669334000\n",
    "                                                              ##  0.079003    0.84672240000\n",
    "                                                               #0.093215     0.8457\n",
    "                                                               #0.078988   0.84892\n",
    "                                                               #0.078988   0.84897390000\n",
    "                                                               #0.84902996000\n",
    "                                                               #0.066291  0.84906185000\n",
    "                                                               #0.065589  0.84901553000\n",
    "                                                               #0.068688  0.84901680000\n",
    "                                                               #0.166649  0.84985\n",
    "                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_result=pd.read_csv('lgb_final_result.csv')['发电量']\n",
    "ID=pd.read_csv('lgb_final_result.csv')['ID']\n",
    "xgb_result=pd.read_csv('reuslt_xgb1.csv')['发电量']\n",
    "lstm_result=pd.read_csv('result_lstm.csv',header=None)[1]\n",
    "\n",
    "submission = pd.DataFrame({'ID': ID, '发电量': lgb_result*0.4+xgb_result*0.3+lstm_result*0.3})\n",
    "submission.to_csv('DataInsight_result.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
